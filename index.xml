<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Skanda Vaidyanath</title>
    <link>https://skandavaidyanath.github.io/</link>
      <atom:link href="https://skandavaidyanath.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Skanda Vaidyanath</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 06 Oct 2022 18:07:14 -0700</lastBuildDate>
    <image>
      <url>https://skandavaidyanath.github.io/img/icon-192.png</url>
      <title>Skanda Vaidyanath</title>
      <link>https://skandavaidyanath.github.io/</link>
    </image>
    
    <item>
      <title>Introduction: Why RL?</title>
      <link>https://skandavaidyanath.github.io/courses/rl-course/post1/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://skandavaidyanath.github.io/courses/rl-course/post1/</guid>
      <description>&lt;p&gt;Hi and welcome to the first post of this RL course! In this post, my aim will be to introduce the idea of RL to you and talk about the problems it solves and why it is important.&lt;/p&gt;
&lt;p&gt;Reinforcement learning is like that little-known cousin of supervised learning and unsupervised learning. Or at least it was for the longest time. RL has actually been around a really long time and I would highly recommend reading the RL book for a more detailed account on the history of RL. But in recent times, its been gaining a lot of attention, mainly due to the conquests of &lt;a href=&#34;https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go&#34;&gt;DeepMind&amp;rsquo;s AlphaZero&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But having said that, people still don&amp;rsquo;t quite know what RL is yet and don&amp;rsquo;t know how and when to use it. So as a part of this introductory blog, I will try to answer three questions that are often asked about RL.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What is Reinforcement Learning?&lt;/li&gt;
&lt;li&gt;How is it different from Supervised learning or Unsupervised learning?&lt;/li&gt;
&lt;li&gt;What problems can it solve?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And lets begin!&lt;/p&gt;
&lt;h2 id=&#34;what-is-reinforcement-learning&#34;&gt;What is Reinforcement Learning?&lt;/h2&gt;
&lt;p&gt;Reinforcement learning is a paradigm of Machine Learning (ML). The most general way to divide ML into three parts would be as Supervised learning (SL), Unsupervised learning (USL) and Reinforcement Learning. But most people only talk about SL and USL when they talk about ML.
So my first job is to explain why the third paradigm is important and how it is different from the first two.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize some notion of cumulative reward.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The above definition is taken from Wikipedia. 
The definition speaks about &amp;ldquo;agents&amp;rdquo; taking &amp;ldquo;actions&amp;rdquo; in &amp;ldquo;environments&amp;rdquo; to maximize &amp;ldquo;rewards&amp;rdquo;. But what does all this mean? Lets break it down, but before that, here is a simpler definition.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Reinforcement learning is simply learning by trial and error.&lt;/p&gt;
&lt;/blockquote&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://skandavaidyanath.github.io/img/post1_rl.png&#34; data-caption=&#34;The RL Setup. Source: Google Images&#34;&gt;
&lt;img src=&#34;https://skandavaidyanath.github.io/img/post1_rl.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The RL Setup. Source: Google Images
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Think about how you started learning to ride a bike. You probably tried many different things (turning the handle-bar in different directions, trying to pedal backwards) and continued doing more of what worked (&amp;ldquo;worked&amp;rdquo; in this context probably means &amp;ldquo;did not fall and moved in the intended direction&amp;rdquo;) and less of what didn&amp;rsquo;t. Nobody gave you clear instructions on what to do at each step, you just tried things and they worked (or didn&amp;rsquo;t). In fact, humans gain several skills in the same fashion. Imagine you&amp;rsquo;re playing a brand new video game without reading the instructions or picking up a new sport. Humans learn several tasks by trial and error and that&amp;rsquo;s exactly what we&amp;rsquo;re trying to emulate with RL. Trying to get as close as possible to the way humans learn.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://skandavaidyanath.github.io/img/post1_bicycle.jpg&#34; data-caption=&#34;Learning to ride a bike. Source: Google Images&#34;&gt;
&lt;img src=&#34;https://skandavaidyanath.github.io/img/post1_bicycle.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Learning to ride a bike. Source: Google Images
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;With that intuition, lets take a jab at the Wikipedia definition again. The &amp;ldquo;agent&amp;rdquo; in our biker example is the person trying to learn to ride a bike. The &amp;ldquo;environment&amp;rdquo; is everything that may affect the person riding the bike &amp;ndash; so this could be the road, the traffic, the weather, etc. Remember the environment is dynamic &amp;ndash; the traffic could get heavier, the weather could get rainier, etc. So the agent has to account for the different settings of the environment. As for &amp;ldquo;actions&amp;rdquo;, these are the different decisions the agent can make &amp;ndash; for example, they could be &amp;ldquo;turn left&amp;rdquo;, &amp;ldquo;turn right&amp;rdquo;, etc. The agent must decide based on the state of the environment, what the right action to play is at a given point. And finally &amp;ldquo;rewards&amp;rdquo; is some sort of feedback we get for the series (could be singular) of actions we just took. So we would get a positive reward if we reached our destination and negative if we fell down for example. All these terms will be dealt with more formally in the next post. For now, just make sure you get the intuition.&lt;/p&gt;
&lt;p&gt;So we have an agent that is going to try and take different possible actions in its environment (which is dynamic!) and learn the best actions under each setting of the environment in order to maximise some notion of a cumulative reward (a feedback signal). All this will get clearer as we go along but for now just remember this.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bottomline: RL is just learning by trial and error to pick the right actions depending on the state of the environment.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;how-is-it-different-from-supervised-learning-or-unsupervised-learning&#34;&gt;How is it different from Supervised learning or Unsupervised learning?&lt;/h2&gt;
&lt;p&gt;This is the question I get asked the most about RL. The difference between RL and USL is quite clear. In USL, there is absolutely no form of feedback or supervision whereas in RL we do get some sort of feedback in the form of a reward signal. Hence, USL is often about finding some sort of structure or patterns in data with absolutely no supervision or feedback. This is not what we&amp;rsquo;re dealing with here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So Reinforcement Learning is not Unsupervised learning.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The more pertinent question is &amp;ndash; how is it different from supervised learning? Is RL just SL with class labels given in a different manner?&lt;/p&gt;
&lt;p&gt;Lets look at another very common example. We want to teach our computer to play Tic Tac Toe.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://skandavaidyanath.github.io/img/post1_tic_tac_toe.png&#34; data-caption=&#34;Tic Tac Toe Game Tree. Source: Google Images&#34;&gt;
&lt;img src=&#34;https://skandavaidyanath.github.io/img/post1_tic_tac_toe.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Tic Tac Toe Game Tree. Source: Google Images
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Our data is in the form of several games that have been played from start to finish. If we consider this as training data for our SL model, the only labels we could possibly decipher from these games would be the final outcome &amp;ndash; the winner of the game or if it was a draw. If we are able to somehow encode the game and train a classifier on the data, this SL model would be able to predict the outcome of a game (which is not very useful) but not &lt;em&gt;how to play the game&lt;/em&gt;. Stop for a second, think about all the different potential SL solutions to this problem and convince yourself that this would be an extremely difficult (if even possible) problem to solve if we do not have supervised labels telling us what the best move is for every board position.&lt;/p&gt;
&lt;p&gt;If we wanted to train a SL model to learn &lt;em&gt;how to play the game&lt;/em&gt;, we would need training data in the form of &lt;em&gt;the best move to play at every board position&lt;/em&gt;. But alas, we do not have such information and this is the case in most problems (think about riding a cycle or playing chess or a video game).&lt;/p&gt;
&lt;p&gt;So how does RL solve the problem? Think of the Tic Tac Toe board and the opponent as an environment now (we can convert our series of game descriptions to such a setup) and our agent is trying to learn how to play the game. If we have enough game descriptions or if the agent can experiment by trying different moves in different board positions through &lt;em&gt;trial and error&lt;/em&gt; and observes the final outcome, then the agent can learn the best moves in the different positions! We&amp;rsquo;ll look into how exactly this is possible over the next couple of posts.&lt;/p&gt;
&lt;p&gt;But since our aim was just to show that RL and SL are not the same, we are done here. SL requires &amp;ldquo;step-wise&amp;rdquo; (this is not a technical term and hence is in quotes, but you get the idea) labels to learn &lt;em&gt;how to do a task&lt;/em&gt;. It requires &amp;ldquo;strong&amp;rdquo; supervision. RL can do the same thing with some sort of &amp;ldquo;weak&amp;rdquo;/&amp;ldquo;distant&amp;rdquo;/&amp;ldquo;semi&amp;rdquo;-supervision.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So Reinforcement Learning is not Supervised learning either.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-problems-can-it-solve&#34;&gt;What problems can it solve?&lt;/h2&gt;
&lt;p&gt;All this sounds great but what problems can RL solve? So far we&amp;rsquo;ve spoken about riding bikes and playing chess and video games but are there any significant real-world problems RL can solve?&lt;/p&gt;
&lt;p&gt;As it turns out there are several. RL is also commonly referred to as &lt;strong&gt;Sequential Decision Making&lt;/strong&gt; or &lt;strong&gt;Decision Making under Uncertainty&lt;/strong&gt; or learning through &lt;strong&gt;Interactions&lt;/strong&gt;. This makes it abundantly clear as to why it is not the same as SL or USL. There is no interaction or sequential decision making in SL or USL. There is a need for a paradigm that learns through interactions and not direct class labels in &amp;ldquo;uncertain&amp;rdquo; conditions. When we put it this way, we can think of several applications for RL in the real-world. I&amp;rsquo;ll talk about a few here.&lt;/p&gt;
&lt;p&gt;


  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://skandavaidyanath.github.io/img/post1_pong.jpg&#34; data-caption=&#34;Atari Games: Pong. Source: Google Images&#34;&gt;
&lt;img src=&#34;https://skandavaidyanath.github.io/img/post1_pong.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Atari Games: Pong. Source: Google Images
  &lt;/figcaption&gt;


&lt;/figure&gt;




  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://skandavaidyanath.github.io/img/post1_alphazero.jpg&#34; data-caption=&#34;AlphaZero: Chess. Source: Google Images&#34;&gt;
&lt;img src=&#34;https://skandavaidyanath.github.io/img/post1_alphazero.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    AlphaZero: Chess. Source: Google Images
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;RL gained massive popularity because of &lt;a href=&#34;https://deepmind.com/&#34;&gt;Google DeepMind&lt;/a&gt; and its success at playing Go and Chess and &lt;a href=&#34;https://arxiv.org/abs/1312.5602&#34;&gt;Atari Games&lt;/a&gt; as well but there are several other applications of RL.&lt;/p&gt;
&lt;p&gt;RL is extremely versatile and can be used along with several other common ML areas like Computer Vision (CV) and Natural Language Processing (NLP).&lt;/p&gt;
&lt;p&gt;The best example of using RL with CV is probably self-driving cars. With NLP, it can be used in dialogue systems. Another massive application area is in robotics and control. It can be used to train multi-agent systems, for example, a swarm of drones communicating with each other. One of my favourite applications of RL is personalized learning where an RL agent can design an optimal course for a student with the right number of tests/assignments administered at the right time to encourage maximum learning. There is similar work being done on personalized healthcare as well.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://skandavaidyanath.github.io/img/post1_robotics.jpg&#34; data-caption=&#34;Robotics. Source: Google Images&#34;&gt;
&lt;img src=&#34;https://skandavaidyanath.github.io/img/post1_robotics.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Robotics. Source: Google Images
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;And so there are a ton of different applications that leverage the power of RL. There are also other slightly different uses, for example, RL can be used to &lt;a href=&#34;https://skandavaidyanath.github.io/post/bridging-the-gaps-with-rl/&#34;&gt;overcome non-differentiable steps in deep learning&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;And with that, we&amp;rsquo;ve answered all the questions that we set out to! We get into a lot more details in the &lt;a href=&#34;https://skandavaidyanath.github.io/courses/rl-course/post2/&#34;&gt;next post&lt;/a&gt; so make sure you take a look at that as well.&lt;/p&gt;
&lt;p&gt;Feel free to let me know if you have any feedback!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>RL Fundamentals and MDPs</title>
      <link>https://skandavaidyanath.github.io/courses/rl-course/post2/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://skandavaidyanath.github.io/courses/rl-course/post2/</guid>
      <description>&lt;p&gt;In this post, we&amp;rsquo;ll try to get into the real nitty-gritties of RL and build on the intuition that we gained from the &lt;a href=&#34;https://skandavaidyanath.github.io/courses/rl-course/post1/&#34;&gt;last article&lt;/a&gt;. So we&amp;rsquo;ll bring in some mathematical foundation and then introduce some RL parlance that we will use for the rest of this course. I&amp;rsquo;ll stick to the standard notation and jargon from the RL book.&lt;/p&gt;
&lt;h2 id=&#34;three-challenges&#34;&gt;Three Challenges&lt;/h2&gt;
&lt;p&gt;Before we begin with the mathematical foundations of RL, I&amp;rsquo;d like to point out three challenges in RL we need to account for if we were to come up with RL algorithms of our own. Once again, I&amp;rsquo;m going to move on to a new example so lets take chess this time.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://skandavaidyanath.github.io/img/post1_alphazero.jpg&#34; data-caption=&#34;AlphaZero: Chess. Source: Google Images&#34;&gt;
&lt;img src=&#34;https://skandavaidyanath.github.io/img/post1_alphazero.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    AlphaZero: Chess. Source: Google Images
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;So we want to teach our agent (recall what this means from the previous post) how to play the game of chess. Lets assume we have some sort of reward function in place where we get some small positive rewards for capturing a piece and small negative rewards for losing a piece depending on the importance of the piece (so losing a queen would lead to a negative reward of larger magnitude than losing a pawn). We also have some large positive final reward for winning the game and a large positive negative reward for losing. If you&amp;rsquo;re wondering whether just this large final reward is a sufficient reward function on its own, you&amp;rsquo;re probably right and it probably is, but lets stick to this for the sake of illustration.&lt;/p&gt;
&lt;p&gt;Now assume we have an RL algorithm that can look at several games of chess and the rewards and learn to play chess on its own. What would this algorithm need to account for? We spoke about trial and error being the basis of any RL algorithm in the previous post that is exactly what our algorithm would do as well. It starts playing random moves and when it plays a good move (positive reward), it remembers to play that move the next time it is in a similar situation. This seems fine on the face of it, but there is an issue. Maybe the algorithm found a good move to play at a given position, &lt;em&gt;but what if there was a better move?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We need some way for the algorithm to account for the possibility of there being a better move than the one it has found already. So when we train our agent we need to make sure the agent doesn&amp;rsquo;t greedily play the best move it knows all the time but also plays some different moves, hoping that they may be better than the one it already found. This is called the &lt;em&gt;exploration-exploitation tradeoff&lt;/em&gt; in RL. Usually, RL algorithms tend to explore i.e. play many random moves initially and when the agent is more sure about the best moves under different circumstances, it starts exploiting that knowledge. We will revisit this problem in the next post with another example.&lt;/p&gt;
&lt;p&gt;Lets move on to the next challenge that our RL algorithm will have to account for. Lets say our RL algorithm is learning from a game of chess again where the player sacrifices the queen but goes on to win the game. The RL agent immediately registers a negative reward for the loss of the queen but the large positive reward for winning the game only comes much later. But it is entirely possible that the very queen sacrifice that the RL agent probably classified as a bad move, was the reason for the player winning the game. So it is possible that a move seems like a &amp;ldquo;bad&amp;rdquo; one in the short term but in the long term, could be a very &amp;ldquo;good&amp;rdquo; move. How do we account for this in our algorithm? This is the concept of &lt;em&gt;delayed rewards&lt;/em&gt; and we will deal with a simple yet elegant solution for this as well as we go through this post. We will look at another example for this as well in the next post.&lt;/p&gt;
&lt;p&gt;And finally our RL algorithm should learn to generalize i.e. it should be able to efficiently learn from the data it collects. One of the major drawbacks in RL is that it requires a lot of data to learn so &lt;em&gt;generalization&lt;/em&gt; is important for RL algorithms.&lt;/p&gt;
&lt;p&gt;Note that RL is based on the &lt;em&gt;reward hypothesis&lt;/em&gt; that states that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;All goals can be described by the maximisation of expected cumulative reward&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Think about whether you agree with this statement.&lt;/p&gt;
&lt;p&gt;With that background, lets talk about how RL problems are modeled and get into some math.&lt;/p&gt;
&lt;h2 id=&#34;markov-decision-processes&#34;&gt;Markov Decision Processes&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;The future is independent of the past, given the present&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;


  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://skandavaidyanath.github.io/img/post2_mdp_example.jpg&#34; data-caption=&#34;A Markov Decision Process. Source: here&#34;&gt;
&lt;img src=&#34;https://skandavaidyanath.github.io/img/post2_mdp_example.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    A Markov Decision Process. Source: &lt;a href=&#34;https://randomant.net/reinforcement-learning-concepts/&#34;&gt;here&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;




  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://skandavaidyanath.github.io/img/post2_rl_with_not.jpg&#34; data-caption=&#34;The RL Framework (with some additional details). Source: Google Images&#34;&gt;
&lt;img src=&#34;https://skandavaidyanath.github.io/img/post2_rl_with_not.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The RL Framework (with some additional details). Source: Google Images
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Almost all RL problems can be modeled as a Markov Decision Process (MDP). So what is an MDP? An MDP is a mathematical model that we will be seeing a lot over this course. Most of our RL environments, will be MDPs. An MDP can be defined as a five tuple:
$$\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$$&lt;/p&gt;
&lt;p&gt;Lets take a closer look at what all of this means. Note that in this section, I do not follow the notation in the RL book and follow the notation &lt;a href=&#34;https://ai.stanford.edu/~ang/papers/icml00-irl.pdf&#34;&gt;here&lt;/a&gt; which is quite commonly followed.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;S&lt;/em&gt; : This is the set of states of the MDP. In an RL setting, this would correspond to different settings of the environment i.e. each setting of the environment is a state in the MDP. In the previous post, we spoke about how RL was all about choosing the right actions at the right times i.e. depending on the state of the environment. This is the state we were talking about. A state in chess or tic-tac-toe could be the board positions or while riding a bike could be some combination of the pertinent variables like the angle of the bike with thr ground, the wind speed, etc. The &lt;em&gt;S&lt;/em&gt; variable represents the set of all unique states in the MDP.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;A&lt;/em&gt; : This is the set of all actions of the MDP. We already spoke about actions briefly. Actions describe the possible moves in a game of chess or tic-tac-toe or different arrow keys or buttons in a video game, etc. It represents the different options the agent has and can play at a given point in time. &lt;em&gt;A&lt;/em&gt; represents the set of all unique actions available to the agent.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is worth mentioning that both &lt;em&gt;S&lt;/em&gt; and &lt;em&gt;A&lt;/em&gt; can be finite or infinite sets. Both can also be continuous spaces or discrete spaces, depending on the variables in the state space or the nature of actions. For example, if we have a variable taking real numbered values in the state space, the state space is automatically continuous (and infinite). If our actions are in the form of degree of turning a steering wheel, once again, continuous and infinite action space. A &lt;em&gt;finite MDP&lt;/em&gt; has both finite &lt;em&gt;S&lt;/em&gt; and finite &lt;em&gt;A&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Before we move on to the other symbols, lets get some things clear. Here is another more detailed MDP for your reference.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://skandavaidyanath.github.io/img/post2_mdp.png&#34; data-caption=&#34;Another MDP. Source: Google Images&#34;&gt;
&lt;img src=&#34;https://skandavaidyanath.github.io/img/post2_mdp.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Another MDP. Source: Google Images
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Some of the states of the MDP are designated as start states or initial states and end states or terminal states. An &lt;em&gt;episode&lt;/em&gt; in RL is a sequence of state-action pairs that take the agent from a start state to a terminal state. So the agent starts from one of the intial states, plays an action, goes to the next state and so on until it hits a terminal state and the episode ends. Assume for now that there is such an end i.e. every episode does end after some unknown, finite time. This is a property of &lt;em&gt;finite horizon MDPs&lt;/em&gt; which is what we will stick to in this course.&lt;/p&gt;
&lt;p&gt;Now lets take a look at this MDP in the diagram above. Assume &lt;em&gt;S&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;0&lt;!-- raw HTML omitted --&gt; is your initial state. Notice that &lt;em&gt;a&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;0&lt;!-- raw HTML omitted --&gt; from &lt;em&gt;S&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;0&lt;!-- raw HTML omitted --&gt; has two arrows, one going into &lt;em&gt;S&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;0&lt;!-- raw HTML omitted --&gt; again and another going into &lt;em&gt;S&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;. The numbers on the arrows indicate 0.5 and 0.5 respectively. This means that if an agent plays the action &lt;em&gt;a&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;0&lt;!-- raw HTML omitted --&gt; from &lt;em&gt;S&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;0&lt;!-- raw HTML omitted --&gt;, it has a 0.5 probability that it ends up back in &lt;em&gt;S&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;0&lt;!-- raw HTML omitted --&gt; and a 0.5 probability that it ends up in &lt;em&gt;S&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;. And similarly we have arrows going all over the diagram. Also notice the wiggly arrows &amp;ndash; they&amp;rsquo;re rewards.&lt;/p&gt;
&lt;p&gt;Notationally, we index the sequence of state-action pairs in an episode with a time variable &lt;em&gt;t&lt;/em&gt; so we say an agent plays action &lt;em&gt;a&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;t&lt;!-- raw HTML omitted --&gt; from state &lt;em&gt;s&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;t&lt;!-- raw HTML omitted --&gt; and gets reward &lt;em&gt;r&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;t+1&lt;!-- raw HTML omitted --&gt; for doing so (the reward can be 0). Here, &lt;em&gt;t&lt;/em&gt; starts from 0 and we represent the terminal time-step as &lt;em&gt;T&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Another crucial thing to note is that actions in an MDP have to be instantaneous in nature. That means you take action &lt;em&gt;a&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;t&lt;!-- raw HTML omitted --&gt; from state &lt;em&gt;s&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;t&lt;!-- raw HTML omitted --&gt; and end up in state &lt;em&gt;s&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;t+1&lt;!-- raw HTML omitted --&gt; immediately. There are several actions in real-world problems that may not be instantaneous but we&amp;rsquo;ll deal with them later. For now, assume your actions are instantaneous.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;P&lt;/em&gt; : Now P is the probability function defined as &lt;em&gt;P&lt;/em&gt;(s&lt;!-- raw HTML omitted --&gt;&#39;&lt;!-- raw HTML omitted --&gt;| s, a) which is read as the probability of moving to &amp;ldquo;state s&#39;&amp;rdquo; from &amp;ldquo;state s&amp;rdquo; if the agent plays &amp;ldquo;action a&amp;rdquo;. So for example, &lt;em&gt;P&lt;/em&gt;(&lt;em&gt;S&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;|&lt;em&gt;S&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;0&lt;!-- raw HTML omitted --&gt;,&lt;em&gt;a&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;0&lt;!-- raw HTML omitted --&gt;) = 0.5. This is also called the &lt;em&gt;transition function&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;R&lt;/em&gt; : This is the &lt;em&gt;reward function&lt;/em&gt; or &lt;em&gt;reinforcement function&lt;/em&gt; and is defined as &lt;em&gt;R&lt;/em&gt;(s&lt;!-- raw HTML omitted --&gt;&#39;&lt;!-- raw HTML omitted --&gt;| s, a) which is the reward the agent gets for moving to &amp;ldquo;state s&#39;&amp;rdquo; from &amp;ldquo;state s&amp;rdquo; if the agent plays &amp;ldquo;action a&amp;rdquo;. So for example, &lt;em&gt;R&lt;/em&gt;(&lt;em&gt;S&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;0&lt;!-- raw HTML omitted --&gt;|&lt;em&gt;S&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;1&lt;!-- raw HTML omitted --&gt;,&lt;em&gt;a&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;0&lt;!-- raw HTML omitted --&gt;) = +5. However, the answer may not be as simple as a single number and the reward could be sampled from a probability distribution i.e. &lt;em&gt;R&lt;/em&gt;(s&lt;!-- raw HTML omitted --&gt;&#39;&lt;!-- raw HTML omitted --&gt;| s, a) is a probability distribution that we will try to estimate by interaction with the environment. There are some other forms of reward functions like &lt;em&gt;R&lt;/em&gt;(s,a) or even just &lt;em&gt;R&lt;/em&gt;(s) as well. Note that rewards are always scalars because they are easier to optimize. They are also usually finite.&lt;/li&gt;
&lt;li&gt;$\gamma$ : We spoke about the concept of delayed rewards earlier in the post and we wanted a way to accound for delayed effects of actions. This is where $\gamma$ helps. We define the &lt;em&gt;returns&lt;/em&gt; of an action from a given state as the sum of the &lt;em&gt;discounted rewards&lt;/em&gt; we receive from that state for playing that action. If we started from the state &lt;em&gt;s&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;0&lt;!-- raw HTML omitted --&gt;, the returns would be defined as &lt;em&gt;r&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;1&lt;!-- raw HTML omitted --&gt; + $\gamma$ &lt;em&gt;r&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt; + $\gamma$&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt; &lt;em&gt;r&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;3&lt;!-- raw HTML omitted --&gt; + &amp;hellip; $\gamma$ &lt;!-- raw HTML omitted --&gt;&lt;em&gt;T-1&lt;/em&gt;&lt;!-- raw HTML omitted --&gt; &lt;em&gt;r&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;&lt;em&gt;T&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;. We use the word &amp;ldquo;discounted&amp;rdquo; because $\gamma$ is usually a number between 0 and 1 and with the increasing powers, we give more weight to the immediate rewards than the delayed rewards. Hence, $\gamma$ is also called the discounting factor. The symbol we use for returns from timestep &lt;em&gt;t&lt;/em&gt; is usually &lt;em&gt;G&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;&lt;em&gt;t&lt;/em&gt;&lt;!-- raw HTML omitted --&gt; although some people like using &lt;em&gt;R&lt;/em&gt; as well (&lt;em&gt;r&lt;/em&gt; for reward and &lt;em&gt;R&lt;/em&gt; for returns). We will stick to the former notation. Now going back to the queen sacrifice example, if we were to consider the returns in our algorithm instead of just the immediate reward, we will be able to account for the delayed positive effect and not just the immediate negative effect.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But with all of that information, we still haven&amp;rsquo;t covered that blockquote at the start of this section. No, that wasn&amp;rsquo;t a quote from Avengers: Endgame and it is the most important takeaway from this post. So what does it mean? Mathematically, it means this:&lt;/p&gt;
&lt;p&gt;$$P(s_{t+1}|s_t,a) = P(s_{t+1}|s_t,s_{t-1},s_{t-2},s_{t-3},&amp;hellip;,a)$$&lt;/p&gt;
&lt;p&gt;But what does that mean intuitively? It means the probability of going to state &lt;em&gt;s&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;t+1&lt;!-- raw HTML omitted --&gt; from state &lt;em&gt;s&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;t&lt;!-- raw HTML omitted --&gt; under the action &lt;em&gt;a&lt;/em&gt; is independent of how we got to state &lt;em&gt;s&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;t&lt;!-- raw HTML omitted --&gt; in the first place. If you&amp;rsquo;re thinking this isn&amp;rsquo;t very realistic, you&amp;rsquo;re right but this type of modeling works in most cases and is called &lt;em&gt;The Markov Property&lt;/em&gt;. The future is independent of the past, given the present i.e. the action from the current state only depends on the current state and not how we got to this state in the first place.&lt;/p&gt;
&lt;p&gt;We make another assumption of stationarity which means:&lt;/p&gt;
&lt;p&gt;$$P(s_{t+1}=s&#39;,r_{t+1}=r|s_{t}=s,a_{t}=a) = P(s&#39;,r|s,a)$$&lt;/p&gt;
&lt;p&gt;This means that the timestep variable doesn&amp;rsquo;t matter &amp;ndash; no matter at what point we are in the trajectory, given the state &lt;em&gt;s&lt;/em&gt; and and action &lt;em&gt;a&lt;/em&gt;, we always end up sampling the next state &lt;em&gt;s&#39;&lt;/em&gt; and the reward &lt;em&gt;r&lt;/em&gt; from the same distribution. This is a property of &lt;em&gt;stationary&lt;/em&gt; MDPs.&lt;/p&gt;
&lt;p&gt;And with that, we&amp;rsquo;ve covered MDPs and how to model RL problems. With this background, I also recommend reading &lt;a href=&#34;https://skandavaidyanath.github.io/post/modeling-rl-problems/&#34;&gt;this&lt;/a&gt;. But with that definition, we still haven&amp;rsquo;t accounted for the exploration-exploitation tradeoff. So in the &lt;a href=&#34;https://skandavaidyanath.github.io/courses/rl-course/post3/&#34;&gt;next post&lt;/a&gt;, we&amp;rsquo;ll introduce a few more symbols and definitions before we get cracking with our very first RL algorithm!&lt;/p&gt;
&lt;p&gt;Once again, let me know if you have any feedback or suggestions.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html&#34;&gt;A (Long) Peek into Reinforcement Learning by Lilian Weng&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html&#34;&gt;David Silver&amp;rsquo;s RL Course at UCL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;For MDP notation, &lt;a href=&#34;https://ai.stanford.edu/~ang/papers/icml00-irl.pdf&#34;&gt;Algorithms for Inverse Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Policies, Value Functions and the Bellman Equation</title>
      <link>https://skandavaidyanath.github.io/courses/rl-course/post3/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://skandavaidyanath.github.io/courses/rl-course/post3/</guid>
      <description>&lt;p&gt;In the last post, we discussed the three major challenges RL algorithms need to tackle and then went on to Markov Decision Processes (MDPs). We&amp;rsquo;ll start this post with a quick recap of some important concepts from the last post and then move on to new ideas.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://skandavaidyanath.github.io/img/post2_mdp.png&#34; data-caption=&#34;An MDP. Source: Google Images&#34;&gt;
&lt;img src=&#34;https://skandavaidyanath.github.io/img/post2_mdp.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    An MDP. Source: Google Images
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;An MDP formally describes the environment for RL when the environment is fully observable i.e. we know all the values of the relevant state variables. In other words, the current state completely characterises the process. Almost all RL problems are formulated as MDPs. This is the same MDP we saw in the previous post. We already discussed the different components of an MDP i.e. the states, the actions, the transition function, reward function and the discount factor $\gamma$. If you need to refresh your memory about any of these, refer back to the previous post before you go ahead.
Here is the Markov property once again for your reference. The future is independent of the past given the present and the state captures all relevant information from the history. This means once we know what state we are in, we don&amp;rsquo;t care how we got to the state in the first place i.e. the state is a sufficient statistic of the future.&lt;/p&gt;
&lt;p&gt;$$P(s_{t+1}|s_t,a) = P(s_{t+1}|s_t,s_{t-1},s_{t-2},s_{t-3},&amp;hellip;,a)$$&lt;/p&gt;
&lt;h2 id=&#34;more-on-delayed-rewards&#34;&gt;More on Delayed Rewards&lt;/h2&gt;
&lt;p&gt;Lets quickly recap the idea of delayed rewards once again. Look at the MDP below (and please ignore all religious connotations that might be implied). Clearly in the MDP below, from earth, the right move is to go to Heaven since you get a reward of +1 in perpetuity even though you get a negative reward to make the transition in the first place. Although the transition to hell initially looks like a good move, you get a -1 in perpetuity and hence doesn&amp;rsquo;t work well in the long term.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://skandavaidyanath.github.io/img/post3_heaven_hell_mdp.PNG&#34; data-caption=&#34;Accounting for delayed rewards&#34;&gt;
&lt;img src=&#34;https://skandavaidyanath.github.io/img/post3_heaven_hell_mdp.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Accounting for delayed rewards
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Although this is not a finite horizon MDP (recall what this means from the last post), the idea is that we want to account not just for immediate gains but long term gains as well.
We already saw how the discount factor and returns allow us to account for this. In fact a $\gamma$ close to 0 leads to &amp;ldquo;myopic&amp;rdquo; or &amp;ldquo;short-sighted&amp;rdquo; evaluation and a $\gamma$ close to 1 leads to &amp;ldquo;far-sighted&amp;rdquo; evaluation.
We often pick a value in between 0 and 1 but closer to 1.
We will revisit the returns definition later in this post.&lt;/p&gt;
&lt;h2 id=&#34;more-on-the-explore-exploit-dilemma&#34;&gt;More on the Explore-Exploit dilemma&lt;/h2&gt;
&lt;p&gt;Lets talk about the exploration-exploitation dilemma we referred to in the last post as well. Look at the figure given below.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://skandavaidyanath.github.io/img/post3_explore_exploit.PNG&#34; data-caption=&#34;Exploration vs Exploitation. Source: Lecture 11 of this&#34;&gt;
&lt;img src=&#34;https://skandavaidyanath.github.io/img/post3_explore_exploit.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Exploration vs Exploitation. Source: Lecture 11 of &lt;a href=&#34;http://ai.berkeley.edu/lecture_slides.html&#34;&gt;this&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The exploration vs exploitation dilemma exists in our everyday lives as well. Imagine your favorite restaurant is right around the corner and you&amp;rsquo;ve been there plenty of times. If you go there every day, you would be confident of what you&amp;rsquo;ll get, but you don&amp;rsquo;t know what you&amp;rsquo;re possibly missing. If you tried different places, you could occasionally be visiting a terrible place but you could also find a new place that is better than this one. So it makes sense to explore and try new places but once you have a good idea pf what&amp;rsquo;s in the area, you can start exploiting and go to your favourite place.
This is exactly what we want our agents to do as well in RL. We want them to explore (randomly or based on some strategy) intially and when its sure of the best actions from each state, it can exploit that knowledge to gain high rewards.&lt;/p&gt;
&lt;h2 id=&#34;more-jargon-and-symbols&#34;&gt;More Jargon and Symbols&lt;/h2&gt;
&lt;p&gt;Now that we&amp;rsquo;re done with that recap, lets get on to some new material. This is the most important section of this post and perhaps the entire series so make sure you understand exactly what&amp;rsquo;s going on in this section.&lt;/p&gt;
&lt;h4 id=&#34;returns&#34;&gt;Returns&lt;/h4&gt;
&lt;p&gt;We already discussed what returns are in the previous post, but here is the formal expression&lt;/p&gt;
&lt;p&gt;$$G_t = r_{t+1}+\gamma r_{t+2}+&amp;hellip;r_{T} = \sum_{k=t}^{T-1} \gamma^{k}r_{k+1}$$&lt;/p&gt;
&lt;p&gt;We end the episode at the timestep &lt;em&gt;T&lt;/em&gt; because we assume a finite horizon MDP. Remember that &lt;em&gt;T&lt;/em&gt; is a random variable &amp;ndash; for example when we play tic-tact-toe, the game could end in a different number of moves each time. Each of the independent $r_{k}$ terms are also random variables and hence, $G_{t}$ is also a random variable. Further, since each reward term is usually finite, the returns does not blow up to infinity.
As we have already discussed, the discount factor is a number between 0 and 1 and often close to 1 and is chosen according to the problem at hand. Also recall that $r_{t+1}$ is the reward received by performing action $a_{t}$ at state $s_{t}$ ; $r_{t+1} = R(s_{t}, a_{t})$ where &lt;em&gt;R&lt;/em&gt; is the reward function.
So this means that $G_{t}$ is the returns accumulated from the state $s_{t}$. When we try to come up with RL algorithms we want to try and optimise $G_{t}$ at all points in the trajectory.&lt;/p&gt;
&lt;h4 id=&#34;policy&#34;&gt;Policy&lt;/h4&gt;
&lt;p&gt;$$\pi_t(a|s) = P(a_t=a|s_t=s)$$&lt;/p&gt;
&lt;p&gt;The policy is a mapping from states to actions or action distributions. The above eqation reads as the probability of taking an action &lt;em&gt;a&lt;/em&gt; at time &lt;em&gt;t&lt;/em&gt; given that the state at time &lt;em&gt;t&lt;/em&gt; was &lt;em&gt;s&lt;/em&gt;.
A &lt;em&gt;stationary policy&lt;/em&gt; is time-independent which means we can omit the timestep variable &lt;em&gt;t&lt;/em&gt;. No matter at what point we are in the trajectory, under the policy, if we are at state &lt;em&gt;s&lt;/em&gt;, then the action we take (or the probability distribution over the actions we can take) is the same. So unless we are specifically talking about non-stationary policies, we omit the &lt;em&gt;t&lt;/em&gt; notation.
Another important distinction is between &lt;em&gt;stochastic&lt;/em&gt; and &lt;em&gt;deterministic&lt;/em&gt; policies. A deterministic policy is a mapping $\pi: S \rightarrow A$ which is essentially a function that maps each state to a single action. A stochastic policy on the other hand maps each state to a probability distribution over all the actions. So a deterministic policy can be thought of as a stochstic policy where only the probability of one of the actions is non-zero.&lt;br&gt;
In all RL problems, we want to learn the policy of the problem and we do this by either trying to learn the policy directly or by learning the policy through the following functions.&lt;/p&gt;
&lt;h4 id=&#34;value-function&#34;&gt;Value function&lt;/h4&gt;
&lt;p&gt;$$V^{\pi}(s) = E_{\pi}(G_{t}|s_{t}=s)$$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;V&lt;/em&gt; is the value function of the state associated with the policy $\pi$. It is the expectation of the returns from the state with respect to the policy $\pi$. Note that $G_{t}$ itself depends on $\pi$ as the next reward depends on $\pi$.
Since the &lt;em&gt;V&lt;/em&gt; function is the expectation of the returns from the state, it tells us how good a state is i.e. what is the expected sum of rewards we can accumulate from a given state under the current policy. So it is a measure of &amp;ldquo;goodness&amp;rdquo; of a state.&lt;/p&gt;
&lt;h4 id=&#34;q-function&#34;&gt;Q function&lt;/h4&gt;
&lt;p&gt;$$Q^{\pi}(s,a) = E_{\pi}(G_{t}|s_{t}=s, a_{t}=a)$$&lt;/p&gt;
&lt;p&gt;This is the Q-function and it is almost the same as the value function, except that here, the first action you take from state &lt;em&gt;s&lt;/em&gt; is always &lt;em&gt;a&lt;/em&gt; and then you follow the policy $\pi$ from then on.
Like the value function, the Q-function tells us not only how good a state is but also how good it is to pick each action from the state. So if $Q^{\pi}(s,a1)$ is greater than $Q^{\pi}(s,a2)$ then it is better to choose action &lt;em&gt;a1&lt;/em&gt; versus action &lt;em&gt;a2&lt;/em&gt; in state &lt;em&gt;s&lt;/em&gt; &lt;em&gt;under the given policy&lt;/em&gt;. So it tells us the &amp;ldquo;goodness&amp;rdquo; of each action in the state.
The value function and the Q-function are crucial for several RL algorithms that we will be seeing over the next few posts.&lt;/p&gt;
&lt;h2 id=&#34;the-bellman-equation&#34;&gt;The Bellman Equation&lt;/h2&gt;
&lt;p&gt;If we were able to find the &lt;em&gt;optimal&lt;/em&gt; value function of each state in the MDP or the Q-function of each state-action pair, then we have solved the RL problem. We will define what &amp;ldquo;optimal&amp;rdquo; means in the next section but for now, just understand that it means &amp;ldquo;the best in terms of collecting maximum rewards&amp;rdquo;. Because at each state, we keep moving to the next potential state with the highest value function (or the state with the potential to collect maximum returns). Or in terms of Q-functions, from each state, we play the action with the highest Q-function value from that state. So we&amp;rsquo;re being greedy with our choices and eventually we keep moving across states in the MDP in this manner until the episode ends. Since we move across states with the highest value function, we collect maximum returns along the way (by virtue of the definition of value functions and a similar argument is true for Q-functions).&lt;/p&gt;
&lt;p&gt;But by doing this, we&amp;rsquo;re also learning a mapping from states to actions i.e. a policy. So if we have the estimates of the &lt;em&gt;optimal&lt;/em&gt; value functions, we can estimate the &lt;em&gt;optimal&lt;/em&gt; policy. This can be tricky to understand because we estimate the value functions using a policy and then we extract a policy by being greedy over the value function. But more on this later.&lt;/p&gt;
&lt;p&gt;So if our estimates of the value functions and Q-functions are optimal, we will be gaining the maximum possible reward in the MDP and would have solved the RL problem. But how do we find the optimal Q-function values or value function values? Remember &lt;em&gt;Q&lt;/em&gt; and &lt;em&gt;V&lt;/em&gt; aren&amp;rsquo;t necessarily the optimal values, they are just the values under some policy $\pi$ (which may not be the optimal policy).&lt;/p&gt;
&lt;p&gt;All of this probably seems confusing now but will be much clearer with the math in the images below and in the next section when we introduce some more simple equations. For now, it is important to understand the importance of trying to learn these &amp;ldquo;optimal&amp;rdquo; value functions and Q-functions.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://skandavaidyanath.github.io/img/post3_bellman_expectation_1.PNG&#34; data-caption=&#34;Bellman Expectation Equations (1). THIS IS VERY IMPORTANT!&#34;&gt;
&lt;img src=&#34;https://skandavaidyanath.github.io/img/post3_bellman_expectation_1.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Bellman Expectation Equations (1). THIS IS VERY IMPORTANT!
  &lt;/figcaption&gt;


&lt;/figure&gt;




  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://skandavaidyanath.github.io/img/post3_bellman_expectation_2.PNG&#34; data-caption=&#34;Bellman Expectation Equations (2). THIS IS VERY IMPORTANT!&#34;&gt;
&lt;img src=&#34;https://skandavaidyanath.github.io/img/post3_bellman_expectation_2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Bellman Expectation Equations (2). THIS IS VERY IMPORTANT!
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Note that the Bellman Expectation Equations form a system of equations &amp;ndash; one equation for every state. There are as many equations as there are number of states. The variables are $V^{\pi}(s)$ for all &lt;em&gt;s&lt;/em&gt;. 
So we have &lt;em&gt;n&lt;/em&gt; equations for &lt;em&gt;n&lt;/em&gt; states and this system of equations will always have a unique solution because the transition matrix is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Stochastic_matrix&#34;&gt;stochastic matrix&lt;/a&gt;.
The equations are linear and they are solvable.&lt;/p&gt;
&lt;h2 id=&#34;optimality-equations&#34;&gt;Optimality Equations&lt;/h2&gt;
&lt;p&gt;The aim of RL is to learn an optimal policy i.e. an optimal mapping from states to actions that can allow the agent to accumulate maximum rewards as it goes on a trajectory (governed by this policy) through the MDP. We want a $\pi$ such that for no other $\pi$ can we get better expected returns.&lt;/p&gt;
&lt;p&gt;$\pi^{*} = \underset{\pi}{\operatorname{argmax}} E_{\pi}[G_{t}|s_{t}=s] \forall s$&lt;/p&gt;
&lt;p&gt;And this can be written as:&lt;/p&gt;
&lt;p&gt;$\pi^{*} = \underset{\pi}{\operatorname{argmax}} V^{\pi}(s) \forall s$&lt;/p&gt;
&lt;p&gt;Here, $\pi^{*}$ is the optimal policy. The $\forall s$ tells us that this is true at any starting state or even any state in the trajectory. Note that there may be more than one optimal policy and in that case, we will take any one of them.
However, &lt;em&gt;even though there are several possible optimal policies, there is only one possible setting for the optimal value functions&lt;/em&gt;, i.e. there may be several optimal $\pi^{*}$ but there can be only one optimal $V^{\pi^{*}}$. Think about this for a second and convince yourself that this is in fact true.&lt;/p&gt;
&lt;p&gt;$V^{\pi^{*}} = \underset{\pi}{\operatorname{argmax}} V^{\pi}(s) \forall s = V^{*}$&lt;/p&gt;
&lt;p&gt;$V^{*}$ is the optimal value function. The existence of such an optimal value function and hence an optimal policy that is able to reach the maximum value function &lt;em&gt;for all states&lt;/em&gt; has been proven in finite MDPs (it gets trickier in infinite MDPs but still holds for some settings). The proof is quite intricate but I have attached some notes from this &lt;a href=&#34;https://nptel.ac.in/courses/106106143/&#34;&gt;NPTEL course&lt;/a&gt; in the reference section for the interested reader.&lt;/p&gt;
&lt;p&gt;The math in the following images talk about the Bellman Optimality Equations. Once again, this is very important for the rest of the course.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://skandavaidyanath.github.io/img/post3_bellman_optimality_1.PNG&#34; data-caption=&#34;Bellman Optimality Equations (1). THIS IS VERY IMPORTANT!&#34;&gt;
&lt;img src=&#34;https://skandavaidyanath.github.io/img/post3_bellman_optimality_1.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Bellman Optimality Equations (1). THIS IS VERY IMPORTANT!
  &lt;/figcaption&gt;


&lt;/figure&gt;




  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://skandavaidyanath.github.io/img/post3_bellman_optimality_2.PNG&#34; data-caption=&#34;Bellman Optimality Equations (2). THIS IS VERY IMPORTANT!&#34;&gt;
&lt;img src=&#34;https://skandavaidyanath.github.io/img/post3_bellman_optimality_2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Bellman Optimality Equations (2). THIS IS VERY IMPORTANT!
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Now if we could solve the Bellman Optimality Equations, we would be done but unfortunately because of the &lt;em&gt;max&lt;/em&gt; operator, the equations are non-linear and cannot be solved easily.
So we need to think about how we can solve this problem.&lt;/p&gt;
&lt;p&gt;Phew! That was a long post with a lot of information to take in but it is extremely important that you understand the ideas in this post clearly. In the next post, we&amp;rsquo;ll move on to our first RL algorithm so make sure you check it out!&lt;/p&gt;
&lt;p&gt;Let me know if you have any feedback or suggestions.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;!-- raw HTML omitted --&gt;Proof of the existence of an optimal value function in finite MDPs&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>LISA: Learning Interpretable Skill Abstractions from Language</title>
      <link>https://skandavaidyanath.github.io/publication/lisa/</link>
      <pubDate>Thu, 06 Oct 2022 18:07:14 -0700</pubDate>
      <guid>https://skandavaidyanath.github.io/publication/lisa/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Jigsaw: Large Language Models meet Program Synthesis</title>
      <link>https://skandavaidyanath.github.io/publication/icse-2022/</link>
      <pubDate>Mon, 27 Dec 2021 13:25:09 -0800</pubDate>
      <guid>https://skandavaidyanath.github.io/publication/icse-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Using Reinforcement Learning to Manage Communications Between Humans and Artificial Agents in an Evacuation Scenario</title>
      <link>https://skandavaidyanath.github.io/publication/flairs-2021/</link>
      <pubDate>Mon, 27 Dec 2021 13:21:24 -0800</pubDate>
      <guid>https://skandavaidyanath.github.io/publication/flairs-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Jigsaw: Large Language Models meet Program Synthesis</title>
      <link>https://skandavaidyanath.github.io/project/jigsaw/</link>
      <pubDate>Mon, 27 Dec 2021 12:44:33 -0800</pubDate>
      <guid>https://skandavaidyanath.github.io/project/jigsaw/</guid>
      <description>&lt;p&gt;Used program synthesis techniques to generate code from multi-modal user input using large language models like GPT-3. Developed a Jupyter notebook extension to generate code from user commands and I/O examples for the Pandas library in Python. This project was done under the guidance of &lt;a href=&#34;https://www.microsoft.com/en-us/research/people/sriram/&#34;&gt;Dr. Sriram Rajamani&lt;/a&gt; at Microsoft Research, India.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Jigsaw: Large Language Models meet Program Synthesis</title>
      <link>https://skandavaidyanath.github.io/talk/msr/</link>
      <pubDate>Sun, 11 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://skandavaidyanath.github.io/talk/msr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Overview of Bandits</title>
      <link>https://skandavaidyanath.github.io/post/bandits/</link>
      <pubDate>Tue, 21 Jan 2020 06:02:24 +0530</pubDate>
      <guid>https://skandavaidyanath.github.io/post/bandits/</guid>
      <description>&lt;p&gt;In this post, we will take a look at the bandit problem and discuss some solution strategies. This is a fairly introductory overview so a basic understanding of probability should be enough to get through this one. Most of the posts on my page talk about RL and various topics related to RL and this post is no different. If you are already familiar with some RL, then the best way to understand bandits is as a simplified RL problem. If you aren&amp;rsquo;t familiar with RL, we&amp;rsquo;ll start from scratch anyway and try to make the link with RL at the end of this post. And with that, lets begin.&lt;/p&gt;
&lt;h2 id=&#34;what-are-bandits&#34;&gt;What are Bandits?&lt;/h2&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;not_this_bandit.jpg&#34; data-caption=&#34;NOT This Bandit. Source: Google Images&#34;&gt;
&lt;img src=&#34;not_this_bandit.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    NOT This Bandit. Source: Google Images
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Multi-armed bandits or bandits are like a slot machine. An arm of a bandit is like the lever of the slot machine that you pull to get some sort of &lt;em&gt;reward&lt;/em&gt; or &lt;em&gt;payoff&lt;/em&gt;.
So imagine now that there is a slot machine with multiple arms. Each arm has an asociated probability distribution of the payoff that it might give. For simplicity, let us assume all of those distributions are Bernoulli distributions for now (although they may be any distribution). So every time you pull an arm &lt;em&gt;a&lt;/em&gt;, you get a payoff 1 with probability &lt;em&gt;p&lt;/em&gt; and 0 with probability 1-&lt;em&gt;p&lt;/em&gt;. Now every arm has a similar Bernoulli reward distribution with different &lt;em&gt;p&lt;/em&gt; values. If you are familiar with the Bernoulli distribution and how to calculate expectations, you&amp;rsquo;ll quickly realize that the expected payoff for arm &lt;em&gt;a&lt;/em&gt; is &lt;em&gt;p&lt;/em&gt;.
Our job is to find and play the arm that would give us the maximum payoff. Now if we knew the distribution associated with each arm and hence the expected payoff for each arm, this would be a trivial problem. All we have to do is keep playing the arm with the highest expected payoff and this is the best we can do (in the Bernoulli case, this will be the arm with the highest probability of giving us a payoff of one). But the catch is that &lt;em&gt;we do not know the distributions associated with each arm&lt;/em&gt;. Now, how do we solve the problem?&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;octopus.png&#34; data-caption=&#34;The Bandit Problem. Source: Google Images&#34;&gt;
&lt;img src=&#34;octopus.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The Bandit Problem. Source: Google Images
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;stationary-and-non-stationary-bandit-problems&#34;&gt;Stationary and Non-stationary bandit problems&lt;/h2&gt;
&lt;p&gt;Before we start talking about the possible solution strategies for the bandit problem, I&amp;rsquo;d like to quickly point out the distinction between stationary bandits and non-stationary bandits. We spoke about each arm having an associated probability distribution for giving out rewards. If this probability distribution is fixed over time, we call it a stationary bandit problem. If it changes over time, we call it a non-stationary bandit problem. In this post, we will only talk about stationary bandits and their solutions. In fact, many of the solutions to the stationary bandit problem can be adapted for the non-stationary case as well but we will not deal with them here.&lt;/p&gt;
&lt;h2 id=&#34;solution-strategies&#34;&gt;Solution Strategies&lt;/h2&gt;
&lt;p&gt;Note: Unless otherwise specified, we will stick to Bernoulli bandits for the sake of simplicity.&lt;/p&gt;
&lt;h3 id=&#34;the-naive-solution&#34;&gt;The Naive Solution&lt;/h3&gt;
&lt;p&gt;You&amp;rsquo;ve probably already thought of it by now but the easiest solution to the problem is to pull each arm many, many times and using the law of large numbers, estimate the expected payoff of each arm and then keep playing the arm with maximum payoff.
This will work but it will take a long time and several arm pulls to get a good estimate of the expected payoff so this is inefficient. How can we do better?&lt;/p&gt;
&lt;p&gt;Before we get into some more intricate solutions, we will need a little bit more background. From now on, we will refer to the true expected payoff of arm &lt;em&gt;a&lt;/em&gt; as &lt;em&gt;q&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt;(&lt;em&gt;a&lt;/em&gt;) and the expected payoff for arm &lt;em&gt;a&lt;/em&gt; that we have computed as &lt;em&gt;q&lt;/em&gt;(&lt;em&gt;a&lt;/em&gt;). So in our initial example, &lt;em&gt;q&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt;(&lt;em&gt;a&lt;/em&gt;) = &lt;em&gt;p&lt;/em&gt;, but we do not know this value or any other &lt;em&gt;q&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt;(&lt;em&gt;a&lt;/em&gt;) value. We only know the &lt;em&gt;q&lt;/em&gt; values which are our estimates of the true &lt;em&gt;q&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt;(&lt;em&gt;a&lt;/em&gt;) values and we want to get our estimates as accurate as possible so that we can eventually pick and play the best arm all the time. One simple example of finding &lt;em&gt;q&lt;/em&gt; values is by taking an average reward obtained from each arm (in the Bernoulli case this will be between 0 and 1).&lt;/p&gt;
&lt;p&gt;So its quite clear that our naive solution is inadequate to solve the problem. Is there even another way? As it turns out, there is. We don&amp;rsquo;t want to play so many arms before we find the best arm, so we need to be smart about which arms we want to play and when. Bandit algorithms and RL algorithms as well follow this extremely crucial strategy called the &lt;em&gt;exploration-exploitation strategy&lt;/em&gt;. The idea now is to explore all the arms of the bandit to try and get an idea of their reward distributions whilst also exploting the information we have by playing the arm with the maximum &lt;em&gt;q&lt;/em&gt; value. This is the expore-exploit dilemma that algorithms must handle. We want to explore as many arms as possible while trying to increase our payoff by playing the best possible arm (according to us, at the time). Now there are several clever exploration strategies that we will talk about in this post but the idea behind all of them is the same. Why can&amp;rsquo;t we just constantly exploit? This would be fine if the best arm according to us is the true best arm, but if we&amp;rsquo;re wrong, then we will constantly play a sub-optimal arm and never learn to play the best arm. Hence, exploration is crucial and you will find that smart exploration will help us find the best arm much faster than our previous naive solution.&lt;/p&gt;
&lt;p&gt;We are going to define a new term now called &lt;em&gt;regret&lt;/em&gt;. Lets assume that we will be able to eventually solve the bandit problem and start playing the right arm all the time. In fact, we have already seen a simple solution to do it and we have spoken about how exploration can improve that solution. The only question is, &lt;em&gt;when&lt;/em&gt;? The sooner we find the best arm, the better our bandit algorithm is. We just spoke about the importance of exploration and how it is going to help us find the best arm quickly. There are several different ways to explore and some strategies take us very close to the optimal payoff very quickly but take a long time to reach &lt;em&gt;the&lt;/em&gt; optimal payoff. Some other strategies may increase the payoff slowly but eventually reach &lt;em&gt;the&lt;/em&gt; optimal payoff faster than the former strategy. In either case, the diagram of how the reward we get increases with time is as follows. The curve itself could be steeper or flatter but the shape is similar. The policy $\pi$ is nothing but a mapping of actions to the probability of playing that action. So $\pi_{t}(a)$ is the probability of playing action a. The policy is subscripted with &lt;em&gt;t&lt;/em&gt; because it changes with time. &lt;em&gt;Regret&lt;/em&gt;, the region shown in the diagram, is the additional reward you could have gained if you had known the best arm from the very beginning. One of the key goals of bandit algorithms is to minimize regret. At any time &lt;em&gt;t&lt;/em&gt;, the regret at the time is proportional to $logt$ but we can play around with the constant (in other words, $logt$ is the lower bound).&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;regret.png&#34; data-caption=&#34;Regret. Source: Google Images&#34;&gt;
&lt;img src=&#34;regret.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Regret. Source: Google Images
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;epsilon-greedy-exploration-and-softmax-exploration&#34;&gt;Epsilon-greedy exploration and Softmax Exploration&lt;/h3&gt;
&lt;p&gt;Lets begin with a simple exploration policy. $\epsilon$-greedy exploration is the simplest exploration policy but is also very important because it is used a lot in the full RL problem as well. It says be greedy and pick the best arm with probability $1-\epsilon$ and pick a random arm (from all the arms including the best arm) with probability $\epsilon$. But we don&amp;rsquo;t want to constantly explore. To eventually converge to the best arm, we need to stop exploring. A good idea is to start with a &amp;ldquo;high&amp;rdquo; (perhaps close to 1) value of $\epsilon$ and keep reducing at the rate of $1/t$ where &lt;em&gt;t&lt;/em&gt; is time.&lt;/p&gt;
&lt;p&gt;But we have a problem with this strategy. If we know after a point that some arms are terrible, we don&amp;rsquo;t want to keep playing them. Also if we have an arm with a slightly higher &lt;em&gt;q&lt;/em&gt; value than  the second best arm, the second best arm is weighted equally with all the other inferior arms. We may want to explore that second best arm a little more to make sure it isn&amp;rsquo;t the best arm. All this is a long preamble to say that we want to weight our exploration by the &lt;em&gt;q&lt;/em&gt;-value of the arm. We can do this by picking arm &lt;em&gt;a&lt;/em&gt; at time &lt;em&gt;t&lt;/em&gt; with probability as follows (assume multi-armed bandit with &lt;em&gt;n&lt;/em&gt; arms):&lt;/p&gt;
&lt;p&gt;$$\frac{q_{t}(a)}{\sum_{b=1}^{n} q_{t}(b)}$$&lt;/p&gt;
&lt;p&gt;But the problem here is that the numerator can be negative and hence the probabilities can be negative. We fix this by using the following formulation. This is called softmax exploration.&lt;/p&gt;
&lt;p&gt;$$\frac{e^{q_{t}(a)/\beta}}{\sum_{b=1}^{n} e^{q_{t}(b)/\beta}}$$&lt;/p&gt;
&lt;p&gt;$\beta$ is called the &lt;em&gt;temperature parameter&lt;/em&gt; and if it is very high, the exploration is completely random. So we start with a high $\beta$ value and keep cooling down and then as $\beta$ tends to 0, we have just the &lt;em&gt;argmax&lt;/em&gt; formulation.&lt;/p&gt;
&lt;p&gt;Both these strategies are used even in the full RL problem. Both these strategies also require us to keep track of the running sum of the &lt;em&gt;q&lt;/em&gt;-values. This equation, which is a form of a stochastic averaging equation does just that. It is fairly straight-forward to derive.&lt;/p&gt;
&lt;p&gt;$$q_t(a) = q_{t-1}(a) + \frac{1}{n_a+1}(r_t - q_{t-1}(a))$$&lt;/p&gt;
&lt;p&gt;Here, $r_t$ is the reward we obtained for playing arm &lt;em&gt;a&lt;/em&gt; at time &lt;em&gt;t&lt;/em&gt; and $n_a$ is the number of times we have played arm &lt;em&gt;a&lt;/em&gt; excluding the current play (the +1 is for the current play).&lt;/p&gt;
&lt;h3 id=&#34;ucb1&#34;&gt;UCB1&lt;/h3&gt;
&lt;p&gt;UCB stands for Upper Confidence Bound. This strategy is supposed to have some good regret bounds. The algorithm is straight-forward and is presented below.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Initialisation: Play each arm once.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Loop: Pull arm j that maximises:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;$$q(j) + 2 \sqrt{\frac{\ln{n}}{n_j}}$$&lt;/p&gt;
&lt;p&gt;Here, &lt;em&gt;n&lt;/em&gt; is the total number of arm plays and $n_j$ is the total number of plays of arm &lt;em&gt;j&lt;/em&gt;. $q_j$ is the expected payoff at time &lt;em&gt;n&lt;/em&gt; for arm &lt;em&gt;j&lt;/em&gt;. &lt;em&gt;Note that this only works for q values bound between 0 and 1. If they are not, then rescale them.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There is a UCB theorem that proves the correctness of the algorithm and how we obtain that magic number in the square root. But I will not get into the proof here. I will give a little bit of intuition though. We want to pick the arm that maximises the &lt;em&gt;q&lt;/em&gt;-value but what is the term in the square root? It is like a confidence bound. UCB says that it isn&amp;rsquo;t just going to use the &lt;em&gt;q&lt;/em&gt; vaues but since it has drawn many samples, it is going to use that information as well to tell what is the confidence with which it can make that estimate. It kind of gives a bound around the &lt;em&gt;q&lt;/em&gt; value. If we have more samples, the bound reduces and as the number of samples tend to infinity, the &lt;em&gt;q&lt;/em&gt; values tend to the true values. So the idea of UCB is to take not just the arm with the max &lt;em&gt;q&lt;/em&gt; value but also the one with the widest range of possible values i.e. the highest upper confidence bound. It also says that since this bound is very big, the arm hasn&amp;rsquo;t been played enough.&lt;/p&gt;
&lt;p&gt;But without going into any of these details, it is a very simple algorithm to implement and get some very good results.&lt;/p&gt;
&lt;h3 id=&#34;naive-pac&#34;&gt;Naive PAC&lt;/h3&gt;
&lt;p&gt;PAC stands for Probably Approximately Correct and often comes with a tag such as $(\epsilon, \delta)$-PAC which means our algorithm will take in input values $\epsilon$ and $\delta$ and give out an arm that satisfies the condition that with probability $1-\delta$, the expected payoff is $\epsilon$ within the best arm.&lt;/p&gt;
&lt;p&gt;The naive PAC algorithm is as follows:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;for each arm &#39;a&#39; do:
   sample &#39;a&#39; &#39;l&#39; times
Let q(a) be the average reward of arm &#39;a&#39;
Output a with the max q(a) value
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Here the magic number &amp;lsquo;l&amp;rsquo; is given as (where &lt;em&gt;K&lt;/em&gt; is the number of arms):
$$l = \frac{2}{\epsilon^2} \ln(\frac{2K}{\delta})$$&lt;/p&gt;
&lt;p&gt;Once again, the algorithm is extremely easy to implement and the only question is how we got the magic number &amp;lsquo;l&amp;rsquo;. There is a theorem that derives this and proves the sample complexity of the algorithm but I will not get into the details here.&lt;/p&gt;
&lt;h3 id=&#34;median-elimination-pac&#34;&gt;Median Elimination PAC&lt;/h3&gt;
&lt;p&gt;This is another PAC algorithm that is as follows. Once again, the algorithm is very easy to implement but much harder to prove so we will not get into the details here.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;med_elim_pac.png&#34; data-caption=&#34;The Median Elimination Algorithm. Source: Google Images&#34;&gt;
&lt;img src=&#34;med_elim_pac.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The Median Elimination Algorithm. Source: Google Images
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;A&lt;/em&gt; is the set of all arms. Median elimination proceeds in rounds. In each round, pull each arm the magic number times. Once we have done that, find the median of all the &lt;em&gt;q&lt;/em&gt; values. Then eliminate all arms whose value is less than the median. Again, change the constants by some magic numbers. Keep going until we have only one arm left.&lt;/p&gt;
&lt;h3 id=&#34;other-solution-strategies&#34;&gt;Other Solution strategies&lt;/h3&gt;
&lt;p&gt;We will quickly talk about a few more solution strategies for the bandit problem. The first one is &lt;em&gt;Thompson Sampling&lt;/em&gt; which is like a Bayesian treatment of the problem. This method is supposed to get better regret bounds than UCB. It involves guessing probability distributions of the different &lt;em&gt;q&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt; values and eventually narrowing down these distributions until they converge.&lt;/p&gt;
&lt;p&gt;The next one is called &lt;em&gt;Policy Search&lt;/em&gt;. As we already mentioned, a policy is a mapping from states to actions in the full RL problem but since we have no states here, it will just be the probability of picking each action at different timesteps. So we represent a policy as $\pi_t(a)$. Now we don&amp;rsquo;t try and find any value functions anymore. We update the policy directly with certain algorithms like Linear Reward Penalty algorithm, Linear Reward $\epsilon$-penalty algorithm and Linear Reward Inaction algorithm. Each of them have very different convergence behaviours and are suitable for different kinds of bandit problems.&lt;/p&gt;
&lt;p&gt;Another common approach that we see a lot in the full RL problem as well is the &lt;em&gt;Policy Gradient&lt;/em&gt; technique where once again, we update the parameters of a policy directly without calculating any intermediate value functions. What do we mean by parameters of a policy? A policy is a probability distribution and we can represent this in many ways, for example, as a softmax function. The exponent in the softmax function can be &amp;ldquo;anything&amp;rdquo; and this &amp;ldquo;anything&amp;rdquo; can involve some parameters that we want to optimise to learn the ideal policy. These parameteres are also called &lt;em&gt;preferences&lt;/em&gt;. The parameters often come from a neural network. We won&amp;rsquo;t talk about policy gradients too much here and we will deal with it in detail in the context of the full RL problem (in the RL course on my page).&lt;/p&gt;
&lt;h2 id=&#34;contextual-bandits----a-step-towards-the-full-rl-problem&#34;&gt;Contextual Bandits &amp;ndash; A Step Towards the Full RL problem&lt;/h2&gt;
&lt;p&gt;We now relax the bandit assumption of &amp;ldquo;no states&amp;rdquo; and we take a step towards the full RL problem. In fact, the bandit problem is called &lt;em&gt;Immediate RL&lt;/em&gt; because as soon as we play the action, we get the feedback in the form of a reward. So contextual bandits are between immediate RL and the full RL problem. But RL problems have a certain sequential dependence that we don&amp;rsquo;t have in the contextual bandit case. This means we don&amp;rsquo;t need to learn the best sequence of states and actions as we do in the full RL problem; the actions we pick now won&amp;rsquo;t affect our future and we don&amp;rsquo;t care about the sequence of actions in our past. All we need to do is pick the best action in each state. So this can be thought of as solving many bandit problems. So if we have 10 states, we have 10 bandit problems and we can solve each of them. These problems are often solved by parametrisation of the states, the actions and the policy (although in many cases this means we parameterise the value function and not the policy).&lt;/p&gt;
&lt;p&gt;And with that we have covered the bandit problem and some of its solutions. Now that we&amp;rsquo;ve gone through all of that, it may be worth it to look at some real-world applications of bandits. You can find some in the paper I have linked below.&lt;/p&gt;
&lt;p&gt;Let me know if you have any suggestions/if I have made any mistakes!&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://nptel.ac.in/courses/106/106/106106143/&#34;&gt;NPTEL Reinforcement Learning Course Weeks 1 and 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1904.10040.pdf&#34;&gt;A Survey on Practical Applications of Multi-Armed and Contextual Bandits (Bouneffouf et. al 2019)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Inverse Reinforcement Learning</title>
      <link>https://skandavaidyanath.github.io/post/inverse-rl-paper/</link>
      <pubDate>Sun, 19 Jan 2020 00:17:50 +0530</pubDate>
      <guid>https://skandavaidyanath.github.io/post/inverse-rl-paper/</guid>
      <description>&lt;p&gt;This is a review of the paper &lt;a href=&#34;https://ai.stanford.edu/~ang/papers/icml00-irl.pdf&#34;&gt;Algorithms for Inverse Reinforcement Learning&lt;/a&gt;. I recommend some reinforcement learning (RL) basics before you read this. The first couple of posts from the RL course on my page might be a good starting point.&lt;/p&gt;
&lt;p&gt;Inverse RL (IRL) is a topic I&amp;rsquo;ve been interested in in recent times so I&amp;rsquo;m excited to write this post. So lets get cracking!&lt;/p&gt;
&lt;h2 id=&#34;the-problem&#34;&gt;The Problem&lt;/h2&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;the_problem.png&#34; data-caption=&#34;The Inverse RL problem. Source: Google Images&#34;&gt;
&lt;img src=&#34;the_problem.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The Inverse RL problem. Source: Google Images
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Reinforcement Learning has allowed researchers to solve several challenging problems without direct supervision but with some sort of distant/ weak supervision or feedback signal. In RL, this feedback signal is in the form of a reward signal. Reward functions are crucial to developing powerful RL models but coming up with good reward functions can be a challenging task. If we were to taake a game like Tic Tac Toe, the reward signal just presents itself based on the result of the game. If it were a video game we were trying to learn, then once again the score in the game provides a solid reward signal. But what about in the case of self-driving cars? With so many factors to consider, it is difficult to come up with a good reward function and even if we do, there may be a better reward function possible. Researchers recognized this issue with RL and decided to come up with a way to &lt;em&gt;learn a reward function&lt;/em&gt;. They wanted to learn a reward function from optimal behaviour. So they would look at a human driving a car, learn a reward function from the demonstration, and then use this reward function to train an RL agent. The problem of extracting the reward fnction from observed optimal behaviour is the problem of Inverse Reinforcement Learning (IRL).&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;problem_definition.PNG&#34; data-caption=&#34;The problem definition. Source: The paper&#34;&gt;
&lt;img src=&#34;problem_definition.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The problem definition. Source: The paper
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;blockquote&gt;
&lt;p&gt;Inverse Reinforcement Learning: Find a reward function to explain observed optimal behaviour&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The paper gives two major motivations to learn such a reward function. One is obvious: to use the reward function to train RL agents. Two, is to use with apprenticeship learning or imitation learning to teach agents.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The reward function and not the policy is the most succinct, robust and transferable definition of a task&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;some-quick-pointers&#34;&gt;Some Quick Pointers&lt;/h2&gt;
&lt;p&gt;Before we get into the meat of the paper, here are some quick pointers to keep in mind.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All reward functions are only functions of the state and not the state and the action. So we have &lt;em&gt;R(s)&lt;/em&gt; and not &lt;em&gt;R(s,a)&lt;/em&gt; everywhere. This is done to simplify the math and the extension is simple. It may help to think of &lt;em&gt;R&lt;/em&gt; as a vector of size &lt;em&gt;N&lt;/em&gt; where &lt;em&gt;N&lt;/em&gt; is the number of states.&lt;/li&gt;
&lt;li&gt;All values of the reward vector are bounded by a magnitude of &lt;em&gt;R&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;max&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;P&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;a&lt;!-- raw HTML omitted --&gt; is a &lt;em&gt;NxN&lt;/em&gt; matrix where &lt;em&gt;P&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;&lt;em&gt;ij&lt;/em&gt;&lt;!-- raw HTML omitted --&gt; is the probability of going from state &lt;em&gt;i&lt;/em&gt; to state &lt;em&gt;j&lt;/em&gt; by playing action &lt;em&gt;a&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;The paper uses the MDP setup for its proofs and arguments. Here are some properties and theorems that they take advantage of.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;mdp_theorems.PNG&#34; data-caption=&#34;MDP Theorems. Source: The paper&#34;&gt;
&lt;img src=&#34;mdp_theorems.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    MDP Theorems. Source: The paper
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;irl-in-finite-state-spaces&#34;&gt;IRL in finite state spaces&lt;/h2&gt;
&lt;p&gt;So we need to find a reward function that explains our optimal behaviour. In this case, let us assume we have a finite state space of size &lt;em&gt;N&lt;/em&gt;. The paper proves the following theorem using the properties above. The proof is quite simple to follow so I won&amp;rsquo;t talk about it here.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;soln_set.PNG&#34; data-caption=&#34;MDP Theorems. Source: The paper&#34;&gt;
&lt;img src=&#34;soln_set.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    MDP Theorems. Source: The paper
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Now what does this theorem tell us? This theorem has now characterized our solution set. We&amp;rsquo;re no longer looking for a needle in a haystack. Initially, our reward vector &lt;em&gt;R&lt;/em&gt; could have been any real vector of size &lt;em&gt;N&lt;/em&gt;. But now, we have some constraints. &lt;em&gt;R&lt;/em&gt; has to satisfy the above condition.&lt;/p&gt;
&lt;p&gt;But the authors point out two issues.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;R&lt;/em&gt;=0 is always a solution. If the reward function is the zero vector, then any policy is an optimal policy and so is our observed policy. The authors point out that this can be alleviated by demanding that our observed policy be the only optimal policy but this doesn&amp;rsquo;t work entirely because although we can get rid of the zero vector now, some vectors arbitrarily close to the zero vector would still be solutions.&lt;/li&gt;
&lt;li&gt;We have characterised our solution set, but there are still several vectors that satisfy this condition. Which of those is our reward function?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To address these issues the authors came up with a linear programming (LP) formulation to find the &amp;ldquo;best&amp;rdquo; &lt;em&gt;R&lt;/em&gt; vector. The authors sought to maximize the sum of the difference between the quality of the optimal action and the next best action.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;lp_term.PNG&#34; data-caption=&#34;The first term is the quality of the best action and the second term is the quality of the next best action. Source: The paper&#34;&gt;
&lt;img src=&#34;lp_term.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The first term is the quality of the best action and the second term is the quality of the next best action. Source: The paper
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;So maximizing the above should give you the reward function but the authors also claim that smaller rewards lead to simpler reward functions and hence want to control the magnitude of the &lt;em&gt;R&lt;/em&gt; vector. To do this, they add a penalty term of $\lambda\Vert R\Vert_{1}$. $\lambda$ is a hyperparameter they control and larger the $\lambda$, the smaller the &lt;em&gt;R&lt;/em&gt; vector norm and simpler the reward function. But this is a trade-off since you also want to maximize the first term.
The authors claim that there is a phase transition point $\lambda_{o}$,
such that if $\lambda &amp;gt; \lambda_{o}$, &lt;em&gt;R&lt;/em&gt;=0 and if $\lambda &amp;lt; \lambda_{o}$, &lt;em&gt;R&lt;/em&gt; is bounded away from 0. So the best $\lambda$ would be a value just below $\lambda_{o}$.&lt;/p&gt;
&lt;p&gt;So the final optimization objective is as follows.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;opti.PNG&#34; data-caption=&#34;Final optimization problem for the finite state space case. Source: The paper&#34;&gt;
&lt;img src=&#34;opti.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Final optimization problem for the finite state space case. Source: The paper
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Note that the summation term running from 1 to &lt;em&gt;N&lt;/em&gt; is so that we maximize across all the states in the MDP. Also note that we use our information of the optimal policy implicitly here since we know the best action &lt;em&gt;a&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;1&lt;!-- raw HTML omitted --&gt; at every step. This can be solved as a LP problem now.&lt;/p&gt;
&lt;h2 id=&#34;irl-in-large-state-spaces-using-linear-function-approximation&#34;&gt;IRL in large state spaces using linear function approximation&lt;/h2&gt;
&lt;p&gt;We now consider a large (possibly infinite) state space. Assume we have &lt;em&gt;n&lt;/em&gt; variables in our state space and so think of &lt;em&gt;R&lt;/em&gt; now as a function $\Re^n \rightarrow \Re$.&lt;/p&gt;
&lt;p&gt;But we don&amp;rsquo;t want to consider all such functions so let us restrict ourselves by only considering functions in the following format. We have used linear function approximation here.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;lfa.PNG&#34; data-caption=&#34;Linear function approximation for the reward function. Source: The paper&#34;&gt;
&lt;img src=&#34;lfa.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Linear function approximation for the reward function. Source: The paper
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The \phi functions are basis functions over the state variables and our job now is to find the $\alpha$ values. Once again, since we&amp;rsquo;re using linear function approximation, we can use LP to solve the problem.&lt;/p&gt;
&lt;p&gt;The paper also shows that the value function under a given policy is also a linear function of the $\alpha$ values (refer to the paper to see why this is true). So now, we can rewrite the equation that characterizes our solution set as the following for the large state space case.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;gen_char_set.PNG&#34; data-caption=&#34;Characterizing the solution set for the large state space case. Source: The paper&#34;&gt;
&lt;img src=&#34;gen_char_set.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Characterizing the solution set for the large state space case. Source: The paper
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;But we have a problem now. This leads to infinitely many constraints to check because the state space could be infinite and we need to check the condition for each one of those states. Remember we had a summation in the final equation in the last section? That would be an infinite summation in this case. However, the authors circumvent this issue algorithmically by just sampling a large number of states and just checking for those states.&lt;/p&gt;
&lt;p&gt;The other issue is that since we have restricted ourselves by using linear function approximation, we may not be able to express all reward functions and hence we&amp;rsquo;ll relax some constraints and pay a penalty when we don&amp;rsquo;t meet the constraints. The final optimization objective is below.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;opti2.PNG&#34; data-caption=&#34;Final optimization problem for the large state space case. Source: The paper&#34;&gt;
&lt;img src=&#34;opti2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Final optimization problem for the large state space case. Source: The paper
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;S&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;o&lt;!-- raw HTML omitted --&gt; is the subsample of states and $p(x) = x$ when $x \geq 0$ and $2x$ when $x &amp;lt; 0$. $\pi$ is the optimal policy. This can be solved with LP now.&lt;/p&gt;
&lt;h2 id=&#34;irl-from-sampled-trajectories&#34;&gt;IRL from sampled trajectories&lt;/h2&gt;
&lt;p&gt;Now, we come to the most interesting and most realistic case. We now try to learn from sampled trajectories from the environment. &lt;strong&gt;We do not require an explicit model of the MDP but we do assume the ability to find an optimal policy under any reward function. We also assume the ability to simulate trajectories in the environment with the optimal policy or any other policy we want.&lt;/strong&gt; Also assume there is only a single start state &lt;em&gt;s&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;o&lt;!-- raw HTML omitted --&gt;. This is not a string assumption as if there are several start states with an initial state distribution, add an additional state and connect it to each of them. This is the most realistic case and is different from the previous cases because we don&amp;rsquo;t have the model of the environment i.e. the &lt;em&gt;P&lt;/em&gt; matrices.&lt;/p&gt;
&lt;p&gt;Once again, &lt;em&gt;R&lt;/em&gt; will be expressed as a linear function approximation in the same form as the previous section. Please refer to the paper and convince yourself that it is possible to use Monte Carlo trajectories to estimate a value function that is also linear in the $\alpha$ values. The math is quite simple and straight-forward. This is important because it allows us to use LP again.&lt;/p&gt;
&lt;p&gt;So now we have the optimization objective as follows.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;opti3.PNG&#34; data-caption=&#34;Final optimization problem for the trajectories case. Source: The paper&#34;&gt;
&lt;img src=&#34;opti3.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Final optimization problem for the trajectories case. Source: The paper
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;p&lt;/em&gt; is as defined in the previous section. But what is &lt;em&gt;k&lt;/em&gt; in the summation? It is the number of policies apart from the optimal policy we are considering at that step of the algorithm. This will be clear in a second. The algorithm is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Start with the optimal policy $\pi^*$ and another random policy $\pi_{1}$. Find the $\alpha$ values that satisfy the above with &lt;em&gt;k&lt;/em&gt;=1. Hence find &lt;em&gt;R&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Now using the &lt;em&gt;R&lt;/em&gt; we just found, find $\pi_{2}$ that maximizes $V^{\pi_{2}}(s_{o})$. This can be done using some RL algorithm.&lt;/li&gt;
&lt;li&gt;Now add $\pi_{2}$ to our current set of policies and optimize the above for $\pi^*$, $\pi_{1}$ and $\pi_{2}$ with &lt;em&gt;k&lt;/em&gt;=2.&lt;/li&gt;
&lt;li&gt;Keep going until you are &amp;ldquo;satisfied&amp;rdquo;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I will not get into the experiments conducted but I would highly recommend that you read the paper since there are some interesting details and observations.&lt;/p&gt;
&lt;h2 id=&#34;future-work&#34;&gt;Future Work&lt;/h2&gt;
&lt;p&gt;The authors plan on finding not just simple reward functions as they have done in this paper i.e. ones with small values but want to do one better to find &amp;ldquo;easy to learn&amp;rdquo; reward functions. I guess this means that &amp;ldquo;simple&amp;rdquo; doesn&amp;rsquo;t always mean &amp;ldquo;easy to learn&amp;rdquo;. They also want a way to be able to account for variation and noise in the state space and action selection process in real world applications. They also hope to find &amp;ldquo;locally consistent&amp;rdquo; reward functions in specific regions of the state space if they find that observed behaviour is far from optimal.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Modeling RL Problems</title>
      <link>https://skandavaidyanath.github.io/post/modeling-rl-problems/</link>
      <pubDate>Thu, 16 Jan 2020 22:12:05 +0530</pubDate>
      <guid>https://skandavaidyanath.github.io/post/modeling-rl-problems/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Disclaimer: The content for this article does not come from any textbook or other reliable sources. They are observations made purely from my very limited experience with RL.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I recommend that you gather some RL basics before you proceed to read this article. The first couple of posts from &lt;a href=&#34;https://skandavaidyanath.github.io/courses/rl-course/&#34;&gt;the course&lt;/a&gt; on my page could be a good start.&lt;/p&gt;
&lt;p&gt;In this article, I&amp;rsquo;m going to talk about something that I haven&amp;rsquo;t seen anywhere before and nobody really talks about it but I&amp;rsquo;m going to take a shot at it. I&amp;rsquo;ve learned RL on my own and hence I&amp;rsquo;ve read several articles on the internet about RL but most of them are about the different algorithms starting from the most basic dynamic programming and going on till the most complex Soft Actor Critic. But having spent some time conducting RL research, I find that nobody really talks about, according to me, the most challenging and interesting problem in RL &amp;ndash; the actual &lt;em&gt;modeling of the problem&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;So what do we mean by &amp;ldquo;modeling the problem&amp;rdquo;? If someone told you to solve the problem of self-driving cars using RL, how would you start? Lets say you decide to go ahead with some MDP framework, what would your states be? What would your actions be? What reward signal would you use? Would you solve the entire &amp;ldquo;self-driving&amp;rdquo; problem at once or would you want to break it down into smaller components? This is what I mean by &amp;ldquo;modeling a problem&amp;rdquo; and in my experience, has turned out to be the most important and challenging part of the bigger problem. Its something that sounds so simple but these early decisions are so important and I strongly believe that solid modeling could lead to better performance than using complex algorithms on weakly structured problems.&lt;/p&gt;
&lt;p&gt;This is in fact one of the things that drew me to RL because I thought modeling problems was extremely interesting and challenging as well. I also couldn&amp;rsquo;t find anything quite similar to this in other domains. Feature selection in supervised learning comes close but there are specific techniques and tests you can conduct to come up with the best set of features given a large feature set. So this seemed like a unique problem specific to the RL world. But if it is such an exciting problem why hasn&amp;rsquo;t anyone written about it? My guess is that there is no simple answer to the question of &amp;ldquo;How to model an RL problem?&amp;rdquo;. The answer is that &amp;ldquo;It depends on the problem&amp;rdquo;. But I do believe there are some very simple guidelines you can follow or rather questions you can ask yourself when you&amp;rsquo;re modeling your problem. So here we go! Five questions that will help you model RL problems. There&amp;rsquo;s one additional question I guess that I&amp;rsquo;m implicitly answering; we will stick to the MDP framework and not talk about SMDPs or POMDPs for now (if you&amp;rsquo;ve never heard of these, good).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What are the actions I need?&lt;/li&gt;
&lt;li&gt;Are my actions instantaneous?&lt;/li&gt;
&lt;li&gt;What is my state space and &lt;em&gt;can I make it smaller&lt;/em&gt;?&lt;/li&gt;
&lt;li&gt;How complex is my problem and can I split it into smaller and easier problems?&lt;/li&gt;
&lt;li&gt;What is the simplest reward function I can use?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So let&amp;rsquo;s go through them one by one.&lt;/p&gt;
&lt;h2 id=&#34;what-are-the-actions-i-need&#34;&gt;What are the actions I need?&lt;/h2&gt;
&lt;p&gt;This is the first question you need to ask yourself when you&amp;rsquo;re handed a problem. What is the agent going to achieve and what actions will it need to achieve that task? This decision is often the easiest to make because this comes along with the problem description. So if the task is to navigate a car from one position of a square grid to another, the directions of movement would probably be the most natural choice for actions. Although this is much harder to see in a problem like Chess or self-driving cars, with a little bit of effort, we can think about all the different actions we have at our disposal. The important thing to keep track of however, is whether the action space is continuous or discrete and if discrete, how many actions you have. This information could be useful to decide if you maybe want to split the problem up into smaller easier problems.
&lt;em&gt;Remember, the more actions you have, the harder the task is.&lt;/em&gt;&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;taxi.png&#34; data-caption=&#34;The Taxi Problem. Source: Google Images&#34;&gt;
&lt;img src=&#34;taxi.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The Taxi Problem. Source: Google Images
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;are-my-actions-instantaneous&#34;&gt;Are my actions instantaneous?&lt;/h2&gt;
&lt;p&gt;Once you&amp;rsquo;ve figured out what your actions are, we can take a closer look at them now. One thing we want to look for is whether the actions are instantaneous or not. Sometimes there could be actions that realistically take time to execute. For example, in the above car-grid problem, if your actions were &amp;ldquo;move the car to the green square&amp;rdquo;, this action could take time to execute realistically. Modeling them as
aneous actions wouldn&amp;rsquo;t be accurate. In the MDP framework, we need all actions to be instantaneous. So what do we do when we have actions that take time? First, try to get rid of them or replace them with simpler instantaneous actions. But this is a trade-off because this could increase the number of actions. Next, think about whether the &amp;ldquo;long&amp;rdquo; action could be a separate problem on its own and that could be modeled as a separate smaller RL problem. But this starts getting into hierarchical RL and I wouldn&amp;rsquo;t get into it unless you&amp;rsquo;re sure about what I&amp;rsquo;m doing.
One thing that I&amp;rsquo;ve done in the past to model actions that take time (that could even vary, every time the action is played) is to model time using probabilities and put a variable in the state space (we&amp;rsquo;ll get to this) indicating that the current action is in progress. So where do the probabilities come into play? Lets say a &amp;ldquo;long&amp;rdquo; action is in progress. Now we keep playing an extra &amp;ldquo;WAIT&amp;rdquo; action or &amp;ldquo;NOP&amp;rdquo; action and flip a coin (not a fair one) and if its heads, the &amp;ldquo;long&amp;rdquo; action ends. This seems convoluted but I&amp;rsquo;ve found that sometimes it could make life a lot simpler by introducing not too much complexity into the state space or action space. Otherwise, simply foraying into SMDPs and hierarchical RL is fine as well if you&amp;rsquo;re confident.&lt;/p&gt;
&lt;h2 id=&#34;what-is-my-state-space-and-can-i-make-it-smaller&#34;&gt;What is my state space and can I make it smaller?&lt;/h2&gt;
&lt;p&gt;So what variables are important for my problem and what goes into my state space? The crucial step here is to think about your actions. What variables do you need to decide what action you want to play in each state? Even thinking about what information a human might need could be useful.
The key is to be minimalistic. Use as few variables as possible and use variables that don&amp;rsquo;t take too many values. Try to maintain a small, discrete state space. This is almost always never possible but its definitely worth the shot. If you can model a problem that you can solve with one of the basic RL algorithms without getting into function approximation (for the uninitiated, think of it as using deep learning), there&amp;rsquo;s nothing like it. But this is difficult and one of the hardest parts of the modeling problem.
It takes a long time at the start of your project and it could get frustrating because you&amp;rsquo;re working with a white board and a marker and not a keyboard and a monitor. But trust me, the effort will be worth it. If you can design a small, simple state space then half the battle is won. At USC, when I was modeling my problem, it took over two weeks but we reduced our state space from over 33 billion to just 1440!&lt;/p&gt;
&lt;h2 id=&#34;how-complex-is-my-problem-and-can-i-split-it-into-smaller-and-easier-problems&#34;&gt;How complex is my problem and can I split it into smaller and easier problems?&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;m afraid there is no easy way to answer this that applies to all RL problems. But let me give you an example that might help. If I&amp;rsquo;m trying to make a robot learn to play tennis, I might want to split the big problem into smaller problems of &amp;ldquo;learning how to serve&amp;rdquo; or &amp;ldquo;learning how to play a forehand&amp;rdquo;, etc. One way of figuring out whether your problem needs breaking down is to check if all the actions make sense with each other i.e. can you play all actions at almost all states? Do you need all the state variables to make a call on whether you want to play an action or can I judge some actions just based on a subset of state variables?
These questions may help you partition your state space and action space into multiple simpler problems that will make it easy to learn. Its not always easy to find these patterns and partitions though. And you also need to make sure there is an easy way to put these sub-problems back together. Nevertheless, if your problem is too complex i.e. too many states and/or too many actions then maybe it is worth spending the time solving multiple problems.&lt;/p&gt;
&lt;h2 id=&#34;what-is-the-simplest-reward-function-i-can-use&#34;&gt;What is the simplest reward function I can use?&lt;/h2&gt;
&lt;p&gt;And finally, my favourite question. Questions 3 and 5 are by far the hardest of the lot and this question is probably the hardest of the lot. Which makes writing the answer so much easier. Researchers have recognized the difficulty of crafting reward functions and created a new field called Inverse Reinforcement Learning just to address the issue. But once again, there are some guidelines you can follow.
Usually, I try to stick to the simplest reward function possible. What do I mean by simplest? Small values and as sparse as possible. So if I&amp;rsquo;m designing a reward function for chess, my rewards would only be at the end of the game and would be a +1, 0, -1 for a win, draw and loss respectively. Another reward function could be for every move played but this would be harder to craft and there&amp;rsquo;s no reason for you to try this until you know the simple reward function doesn&amp;rsquo;t work. But in the problem of chess, it is qite straight-forward but there are other problems where finding an elegant reward function might not be as simple.&lt;/p&gt;
&lt;p&gt;So there you have it. I hope that helped and hopefully when you model your next problem, these tips will help. But thinking about all of this modeling has made me think about inverse reinforcement learning and meta learning a lot more. Is it possible for RL models to learn the optimal design of a problem? Not just the reward function but also the state space and the action space? And if it can, can it learn some common properties that we can learn across all RL problems? These are tough questions that I hope to answer some day.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bridging the Gaps With Reinforcement Learning</title>
      <link>https://skandavaidyanath.github.io/post/bridging-the-gaps-with-rl/</link>
      <pubDate>Mon, 04 Nov 2019 03:10:09 +0530</pubDate>
      <guid>https://skandavaidyanath.github.io/post/bridging-the-gaps-with-rl/</guid>
      <description>&lt;p&gt;In this post, I will be talking about a unique way to use reinforcement learning (RL) in deep learning applications. I definitely recommend brushing up some deep learning fundamentals and if possible, some policy gradient fundamentals as well before you get started with this post.&lt;/p&gt;
&lt;p&gt;Traditionally, RL is used to solve sequential decision making problems in the video game space or robotics space or any other space where there is a concrete RL task at hand. There are several applications in the fields of video games and robotics where the task at hand can be very easily seen as an RL problem and can be modeled appropriately as well. But RL as a technique is quite versatile and can be used in several other domains to train neural networks that are traditionally trained in a supervised fashion. We&amp;rsquo;ll talk about one very important such application in this post. Along the way, I&amp;rsquo;ll also try to convince you that this isn&amp;rsquo;t really a different way to use RL but rather just a different way to look at the traditional RL problem. So with that, lets begin!&lt;/p&gt;
&lt;h2 id=&#34;non-differentiable-steps-in-deep-learning-the-gaps&#34;&gt;Non-differentiable steps in deep learning: The Gaps&lt;/h2&gt;
&lt;p&gt;Sometimes when we&amp;rsquo;re coming up with neural network architectures, we may need to incorporate some non-differentiable operations as a part of our network. Now this is a problem as we can&amp;rsquo;t backpropagate losses through such operation and hence lets call these &amp;ldquo;gaps&amp;rdquo;. So what are some common gaps we come across in neural networks?&lt;/p&gt;
&lt;p&gt;On a side-note, before we start talking about some &amp;ldquo;real gaps&amp;rdquo;, its worth mentioning that the famous ReLU function is a non-differentiable function but we overcome that gap by setting the derivative at 0 to either 1 or 0 and get away with it.&lt;/p&gt;
&lt;p&gt;Now lets take a better example &amp;ndash; variational autoencoders (VAE). Without going into two many details, the VAE network outputs two vectors: a $\mu$ vector and a $\sigma$ vector and it involves a crucial sampling step where we sample from the distribution &lt;em&gt;N($\mu$, $\sigma$)&lt;/em&gt; as a part of the network. Now sampling is a gap as it is a non-differentiable step. You should stop here and convince yourself that this is, in fact true. When we sample, we don&amp;rsquo;t know what the outcome will be and hence the function in not differentiable. So how do they get over this in the VAE case? They use a clever trick.
Instead of sampling from &lt;em&gt;N&lt;/em&gt;($\mu$, $\sigma$), they just rewrite this as $\mu$ + $\sigma$&lt;em&gt;N(0,1)&lt;/em&gt; where they sample from the standard normal function. This neat trick now makes the expression differentiable because we just need the $\mu$ and $\sigma$ quantities to be differentiable and we don&amp;rsquo;t care about the &lt;em&gt;N(0,1)&lt;/em&gt;. Remember that we only need to differentiate with respect to the parameters of our network (brush up some backpropagation basics if you&amp;rsquo;re confused here) and hence we need to differentiate only with respect to $\mu$ and $\sigma$ and not the standard normal distribution. For more details about VAEs read &lt;a href=&#34;https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf&#34;&gt;this post&lt;/a&gt; or &lt;a href=&#34;https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73&#34;&gt;this one&lt;/a&gt;.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;vae.PNG&#34; data-caption=&#34;Variational Autoencoders. Source: here&#34;&gt;
&lt;img src=&#34;vae.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Variational Autoencoders. Source: &lt;a href=&#34;https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf&#34;&gt;here&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;So as it turned out, that wasn&amp;rsquo;t a very good example either but we&amp;rsquo;re starting to understand what we mean by gaps now and how common they are. Some common examples of gaps in networks are sampling operations and the argmax operation. Once again, convince yourself that argmax is not a differentiable function. Assume you have a function that takes argmax of two quantities &lt;em&gt;(x1,x2)&lt;/em&gt;. When &lt;em&gt;x1&lt;/em&gt; &amp;gt; &lt;em&gt;x2&lt;/em&gt;, this has value 0 (zero-indexed) and when &lt;em&gt;x1&lt;/em&gt; &amp;lt; &lt;em&gt;x2&lt;/em&gt;, it has value 1. Say the function is not defined on the &lt;em&gt;x1==x2&lt;/em&gt; line or define it as you wish (0 or 1). Now if you can visualise the 2D plane, you&amp;rsquo;ll see that the function is not differentiable as we move across the &lt;em&gt;x1==x2&lt;/em&gt; line. So argmax isn&amp;rsquo;t differentiable &lt;em&gt;but max is a differentiable function&lt;/em&gt; (recall max pooling in CNNs). Read &lt;a href=&#34;https://www.reddit.com/r/MachineLearning/comments/4e2get/argmax_differentiable/&#34;&gt;this thread&lt;/a&gt; for more details.&lt;/p&gt;
&lt;p&gt;These functions are commonly used in natural language processing (NLP) applications, information retrieval (IR) applications and Computer Vision (CV) applications as well. For example, a sampling function could be used to select words from a sentence based on a probability distribution in an NLP application or an argmax function could be used to find the highest ranked document in an IR application. &lt;a href=&#34;https://jhui.github.io/2017/03/15/Soft-and-hard-attention/&#34;&gt;Hard attention&lt;/a&gt; uses sampling techniques which involves non-differentiable computation.&lt;/p&gt;
&lt;p&gt;So its quite clear that these gaps are common in several deep learning architectures and sometimes, it could even be useful to introduce such a gap in the network intentionally to reap added benefits. The only question is, &lt;em&gt;how do we bridge these gaps?&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;reinforcement-learning-and-policy-gradients-the-bridge&#34;&gt;Reinforcement Learning and Policy Gradients: The Bridge&lt;/h2&gt;
&lt;p&gt;Policy gradients are a class of algorithms in RL. There are several policy gradient algorithms and &lt;a href=&#34;https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html&#34;&gt;this&lt;/a&gt; is a great blog that lists out almost all of them. But without going into too many details, these algorithms work in the policy space by updating the parameters of the policy we&amp;rsquo;re trying to learn. That means we don&amp;rsquo;t necessarily need to find the value function of different states but we can directly alter our policy until we&amp;rsquo;re happy.
The most common policy gradient (PG) algorithm is the REINFORCE which is a Monte Carlo algorithm. This means we run an entire episode and make changes to our policy only at the end of each episode and not at every step. We make these changes based on the returns that we received by taking a given action from a given state in the episode. I skip the derivation of the policy gradient here but it can be found in the link above. The final result is in the image below.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;pg.PNG&#34; data-caption=&#34;The Policy Gradient. Source: here&#34;&gt;
&lt;img src=&#34;pg.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The Policy Gradient. Source: &lt;a href=&#34;https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63&#34;&gt;here&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The key idea here is that in policy gradient methods, we are allowed to &lt;em&gt;sample different actions from a given state and wait till the end of an episode before we make updates to our network&lt;/em&gt;. So if we have a sampling operation as a part of our network, we can introduce a policy gradient and think of it as sampling actions in a given state in an RL framework. A similar procedure can also be followed if we had argmax in place of the sampling operation.&lt;/p&gt;
&lt;p&gt;Consider a neural network now with a gap. The images below are taken from &lt;a href=&#34;http://karpathy.github.io/2016/05/31/rl/&#34;&gt;this blog&lt;/a&gt; on Policy Gradients written by Andrej Karpathy.&lt;/p&gt;
&lt;p&gt;












&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;karpathy1.PNG&#34; data-caption=&#34;Gaps in a neural network. Source: Karpathy&amp;rsquo;s blog&#34;&gt;
&lt;img src=&#34;karpathy1.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Gaps in a neural network. Source: Karpathy&amp;rsquo;s &lt;a href=&#34;http://karpathy.github.io/2016/05/31/rl/&#34;&gt;blog&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;karpathy2.PNG&#34; data-caption=&#34;The sampling operation. Source: Karpathy&amp;rsquo;s blog&#34;&gt;
&lt;img src=&#34;karpathy2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The sampling operation. Source: Karpathy&amp;rsquo;s &lt;a href=&#34;http://karpathy.github.io/2016/05/31/rl/&#34;&gt;blog&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;So now we can train the blue arrows i.e. the differentiable path as usual. But to train the red arrows, we need to introduce a policy gradient and as we sample, we ensure with the help of the policy gradient that we encourage samples that led to a lower loss. In this way, we are &amp;ldquo;training&amp;rdquo; the sampling operation or one could say, propagating the loss through the sampling operation! Note that the updates to the red arrows happen independently than those of the blue arrows.
Note that in the diagrams above, there isn&amp;rsquo;t really a gap per-say because the blue arrows go all the way from start to finish. So there is a differentiable path and a non-differentiable path. A true gap would mean there would be no completely differentiable path at all. In this case, we need to make sure that the loss functions on either side of the gap are &amp;ldquo;in sync&amp;rdquo; and are being optimized in such a way that it facilitates joint training and achieves a common goal. This is often not as easy as it sounds.&lt;/p&gt;
&lt;p&gt;I said at the start that as obscure as it seems, this is still the traditional RL problem we&amp;rsquo;re used to with the MDP and states and actions. We can still look at this entire setup as a traditional RL problem if we think of the inputs to the neural network as the state and the sampling process as sampling different actions from that given state. Now what is the reward function? This depends on what comes after the gap and could be an output from the rest of the network or it could be a completely independent reward function that you came up with as well. So at the end of the day, it is still the same MDP with the traditional setup but just used in a very different way.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep RL for IR applications</title>
      <link>https://skandavaidyanath.github.io/project/mpii/</link>
      <pubDate>Fri, 01 Nov 2019 04:46:38 +0530</pubDate>
      <guid>https://skandavaidyanath.github.io/project/mpii/</guid>
      <description>&lt;p&gt;Used deep reinforcement learning to recommend travel destinations to users based on forum posts and user reviews. Used deep reinforcement learning to improve document retrieval from a corpus via document expansion. This project was a part of my undergraduate thesis at the Max Planck Institute for Informatics under &lt;a href=&#34;https://andrewyates.net/&#34;&gt;Dr. Andrew Yates&lt;/a&gt; and &lt;a href=&#34;https://paramitamirza.com/&#34;&gt;Dr. Paramita Mirza&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Software Bug Prediction</title>
      <link>https://skandavaidyanath.github.io/project/bug-prediction/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://skandavaidyanath.github.io/project/bug-prediction/</guid>
      <description>&lt;p&gt;Developed an efficient NLP-based method to extract features from source code of software projects to detect bugs. This project was done under the guidance of &lt;a href=&#34;https://www.bits-pilani.ac.in/hyderabad/bhanumurthy/Profile&#34;&gt;Prof. N. L. Bhanu Murthy&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Human-Swarm Project</title>
      <link>https://skandavaidyanath.github.io/talk/usc-ict/</link>
      <pubDate>Sun, 21 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://skandavaidyanath.github.io/talk/usc-ict/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Human Swarm Project</title>
      <link>https://skandavaidyanath.github.io/project/usc-ict/</link>
      <pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://skandavaidyanath.github.io/project/usc-ict/</guid>
      <description>&lt;p&gt;Developed a policy to control a swarm of drones that attempts to negotiate with and save civilians from a forest fire and achieved a success rate of over 95% on the task. This project was done under the guidance of &lt;a href=&#34;http://people.ict.usc.edu/~kgeorgila/&#34;&gt;Prof. Kallirroi Georgila&lt;/a&gt; and &lt;a href=&#34;http://ict.usc.edu/profile/david-traum/&#34;&gt;Prof. David Traum&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Brain Decoding</title>
      <link>https://skandavaidyanath.github.io/project/brain-decoding/</link>
      <pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://skandavaidyanath.github.io/project/brain-decoding/</guid>
      <description>&lt;p&gt;This &lt;a href=&#34;https://vamsi-aribandi.github.io/brain_decoding_1/&#34;&gt;blog post&lt;/a&gt; has some interesting details. The code and a PDF report are available in links above. This project was done by &lt;a href=&#34;https://vamsi-aribandi.github.io/&#34;&gt;Vamsi Aribandi&lt;/a&gt; and myself.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Community Question Answering for a distance-learning platform</title>
      <link>https://skandavaidyanath.github.io/project/cqa/</link>
      <pubDate>Thu, 01 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://skandavaidyanath.github.io/project/cqa/</guid>
      <description>&lt;p&gt;Developed deep learning solutions for retrieving similar questions from a large Q&amp;amp;A archive for a distance-learning platform.  This project was done under the guidance of &lt;a href=&#34;https://www.bits-pilani.ac.in/hyderabad/bhanumurthy/Profile&#34;&gt;Prof. N. L. Bhanu Murthy&lt;/a&gt; and &lt;a href=&#34;https://universe.bits-pilani.ac.in/hyderabad/arunamalapati/Profile&#34;&gt;Prof. Aruna Malapati&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Personalized Learning from Job Descriptions</title>
      <link>https://skandavaidyanath.github.io/project/mitacs/</link>
      <pubDate>Sat, 20 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://skandavaidyanath.github.io/project/mitacs/</guid>
      <description>&lt;p&gt;Python notebook and slides for the project are linked above. This project was done as a part of the applications for the MITACS Globalink programme.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Search Engine On a Nuclear Corpus</title>
      <link>https://skandavaidyanath.github.io/project/igcar/</link>
      <pubDate>Tue, 31 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://skandavaidyanath.github.io/project/igcar/</guid>
      <description>&lt;p&gt;The code for this project cannot be made public. However, you can find a technical report and some slides in the links above. This project was done at the Indira Gandhi Centre for Atomic Research (IGCAR).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Developing a Search Engine on a nuclear corpus and outlining a semantic based approach to Entity Profiling from raw text to build a Question Answering system</title>
      <link>https://skandavaidyanath.github.io/talk/igcar/</link>
      <pubDate>Wed, 25 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://skandavaidyanath.github.io/talk/igcar/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
