<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.5.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Skanda Vaidyanath">

  
  
  
    
  
  <meta name="description" content="This is a review of the paper Algorithms for Inverse Reinforcement Learning. I recommend some reinforcement learning (RL) basics before you read this. The first couple of posts from the RL course on my page might be a good starting point.
Inverse RL (IRL) is a topic I&rsquo;ve been interested in in recent times so I&rsquo;m excited to write this post. So lets get cracking!
The Problem    The Inverse RL problem.">

  
  <link rel="alternate" hreflang="en-us" href="https://skandavaidyanath.github.io/post/inverse-rl-paper/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.8565976d7831ba0a0daf18ae5d4ff0e8.css">

  

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://skandavaidyanath.github.io/post/inverse-rl-paper/">

  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Skanda Vaidyanath">
  <meta property="og:url" content="https://skandavaidyanath.github.io/post/inverse-rl-paper/">
  <meta property="og:title" content="Inverse Reinforcement Learning | Skanda Vaidyanath">
  <meta property="og:description" content="This is a review of the paper Algorithms for Inverse Reinforcement Learning. I recommend some reinforcement learning (RL) basics before you read this. The first couple of posts from the RL course on my page might be a good starting point.
Inverse RL (IRL) is a topic I&rsquo;ve been interested in in recent times so I&rsquo;m excited to write this post. So lets get cracking!
The Problem    The Inverse RL problem."><meta property="og:image" content="https://skandavaidyanath.github.io/post/inverse-rl-paper/featured.png">
  <meta property="twitter:image" content="https://skandavaidyanath.github.io/post/inverse-rl-paper/featured.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-01-19T00:17:50&#43;05:30">
    
    <meta property="article:modified_time" content="2020-01-19T00:17:50&#43;05:30">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://skandavaidyanath.github.io/post/inverse-rl-paper/"
  },
  "headline": "Inverse Reinforcement Learning",
  
  "image": [
    "https://skandavaidyanath.github.io/post/inverse-rl-paper/featured.png"
  ],
  
  "datePublished": "2020-01-19T00:17:50+05:30",
  "dateModified": "2020-01-19T00:17:50+05:30",
  
  "author": {
    "@type": "Person",
    "name": "Skanda Vaidyanath"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Skanda Vaidyanath",
    "logo": {
      "@type": "ImageObject",
      "url": "https://skandavaidyanath.github.io/img/icon-512.png"
    }
  },
  "description": "This is a review of the paper Algorithms for Inverse Reinforcement Learning. I recommend some reinforcement learning (RL) basics before you read this. The first couple of posts from the RL course on my page might be a good starting point.\nInverse RL (IRL) is a topic I\u0026rsquo;ve been interested in in recent times so I\u0026rsquo;m excited to write this post. So lets get cracking!\nThe Problem    The Inverse RL problem."
}
</script>

  

  


  


  





  <title>Inverse Reinforcement Learning | Skanda Vaidyanath</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Skanda Vaidyanath</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#post"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/courses/"><span>Courses</span></a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article">

  




















  
  
    
  


<div class="article-container pt-3">
  <h1>Inverse Reinforcement Learning</h1>

  

  



<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/admin/">Skanda Vaidyanath</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Jan 19, 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    9 min read
  </span>
  

  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/categories/reinforcement-learning/">Reinforcement Learning</a></span>
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://skandavaidyanath.github.io/post/inverse-rl-paper/&amp;text=Inverse%20Reinforcement%20Learning" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://skandavaidyanath.github.io/post/inverse-rl-paper/&amp;t=Inverse%20Reinforcement%20Learning" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Inverse%20Reinforcement%20Learning&amp;body=https://skandavaidyanath.github.io/post/inverse-rl-paper/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://skandavaidyanath.github.io/post/inverse-rl-paper/&amp;title=Inverse%20Reinforcement%20Learning" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Inverse%20Reinforcement%20Learning%20https://skandavaidyanath.github.io/post/inverse-rl-paper/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://skandavaidyanath.github.io/post/inverse-rl-paper/&amp;title=Inverse%20Reinforcement%20Learning" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>


  

</div>

  














</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 720px; max-height: 803px;">
  <div style="position: relative">
    <img src="/post/inverse-rl-paper/featured_hu615da993f187b8851e76f2d277012e97_238280_720x0_resize_lanczos_3.png" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <p>This is a review of the paper <a href="https://ai.stanford.edu/~ang/papers/icml00-irl.pdf">Algorithms for Inverse Reinforcement Learning</a>. I recommend some reinforcement learning (RL) basics before you read this. The first couple of posts from the RL course on my page might be a good starting point.</p>
<p>Inverse RL (IRL) is a topic I&rsquo;ve been interested in in recent times so I&rsquo;m excited to write this post. So lets get cracking!</p>
<h2 id="the-problem">The Problem</h2>













<figure>


  <a data-fancybox="" href="the_problem.png" data-caption="The Inverse RL problem. Source: Google Images">
<img src="the_problem.png" alt="" ></a>


  
  
  <figcaption>
    The Inverse RL problem. Source: Google Images
  </figcaption>


</figure>

<p>Reinforcement Learning has allowed researchers to solve several challenging problems without direct supervision but with some sort of distant/ weak supervision or feedback signal. In RL, this feedback signal is in the form of a reward signal. Reward functions are crucial to developing powerful RL models but coming up with good reward functions can be a challenging task. If we were to taake a game like Tic Tac Toe, the reward signal just presents itself based on the result of the game. If it were a video game we were trying to learn, then once again the score in the game provides a solid reward signal. But what about in the case of self-driving cars? With so many factors to consider, it is difficult to come up with a good reward function and even if we do, there may be a better reward function possible. Researchers recognized this issue with RL and decided to come up with a way to <em>learn a reward function</em>. They wanted to learn a reward function from optimal behaviour. So they would look at a human driving a car, learn a reward function from the demonstration, and then use this reward function to train an RL agent. The problem of extracting the reward fnction from observed optimal behaviour is the problem of Inverse Reinforcement Learning (IRL).</p>













<figure>


  <a data-fancybox="" href="problem_definition.PNG" data-caption="The problem definition. Source: The paper">
<img src="problem_definition.PNG" alt="" ></a>


  
  
  <figcaption>
    The problem definition. Source: The paper
  </figcaption>


</figure>

<blockquote>
<p>Inverse Reinforcement Learning: Find a reward function to explain observed optimal behaviour</p>
</blockquote>
<p>The paper gives two major motivations to learn such a reward function. One is obvious: to use the reward function to train RL agents. Two, is to use with apprenticeship learning or imitation learning to teach agents.</p>
<blockquote>
<p>The reward function and not the policy is the most succinct, robust and transferable definition of a task</p>
</blockquote>
<h2 id="some-quick-pointers">Some Quick Pointers</h2>
<p>Before we get into the meat of the paper, here are some quick pointers to keep in mind.</p>
<ul>
<li>All reward functions are only functions of the state and not the state and the action. So we have <em>R(s)</em> and not <em>R(s,a)</em> everywhere. This is done to simplify the math and the extension is simple. It may help to think of <em>R</em> as a vector of size <em>N</em> where <em>N</em> is the number of states.</li>
<li>All values of the reward vector are bounded by a magnitude of <em>R</em><!-- raw HTML omitted -->max<!-- raw HTML omitted --></li>
<li><em>P</em><!-- raw HTML omitted -->a<!-- raw HTML omitted --> is a <em>NxN</em> matrix where <em>P</em><!-- raw HTML omitted --><em>ij</em><!-- raw HTML omitted --> is the probability of going from state <em>i</em> to state <em>j</em> by playing action <em>a</em>.</li>
<li>The paper uses the MDP setup for its proofs and arguments. Here are some properties and theorems that they take advantage of.</li>
</ul>













<figure>


  <a data-fancybox="" href="mdp_theorems.PNG" data-caption="MDP Theorems. Source: The paper">
<img src="mdp_theorems.PNG" alt="" ></a>


  
  
  <figcaption>
    MDP Theorems. Source: The paper
  </figcaption>


</figure>

<h2 id="irl-in-finite-state-spaces">IRL in finite state spaces</h2>
<p>So we need to find a reward function that explains our optimal behaviour. In this case, let us assume we have a finite state space of size <em>N</em>. The paper proves the following theorem using the properties above. The proof is quite simple to follow so I won&rsquo;t talk about it here.</p>













<figure>


  <a data-fancybox="" href="soln_set.PNG" data-caption="MDP Theorems. Source: The paper">
<img src="soln_set.PNG" alt="" ></a>


  
  
  <figcaption>
    MDP Theorems. Source: The paper
  </figcaption>


</figure>

<p>Now what does this theorem tell us? This theorem has now characterized our solution set. We&rsquo;re no longer looking for a needle in a haystack. Initially, our reward vector <em>R</em> could have been any real vector of size <em>N</em>. But now, we have some constraints. <em>R</em> has to satisfy the above condition.</p>
<p>But the authors point out two issues.</p>
<ol>
<li><em>R</em>=0 is always a solution. If the reward function is the zero vector, then any policy is an optimal policy and so is our observed policy. The authors point out that this can be alleviated by demanding that our observed policy be the only optimal policy but this doesn&rsquo;t work entirely because although we can get rid of the zero vector now, some vectors arbitrarily close to the zero vector would still be solutions.</li>
<li>We have characterised our solution set, but there are still several vectors that satisfy this condition. Which of those is our reward function?</li>
</ol>
<p>To address these issues the authors came up with a linear programming (LP) formulation to find the &ldquo;best&rdquo; <em>R</em> vector. The authors sought to maximize the sum of the difference between the quality of the optimal action and the next best action.</p>













<figure>


  <a data-fancybox="" href="lp_term.PNG" data-caption="The first term is the quality of the best action and the second term is the quality of the next best action. Source: The paper">
<img src="lp_term.PNG" alt="" ></a>


  
  
  <figcaption>
    The first term is the quality of the best action and the second term is the quality of the next best action. Source: The paper
  </figcaption>


</figure>

<p>So maximizing the above should give you the reward function but the authors also claim that smaller rewards lead to simpler reward functions and hence want to control the magnitude of the <em>R</em> vector. To do this, they add a penalty term of $\lambda\Vert R\Vert_{1}$. $\lambda$ is a hyperparameter they control and larger the $\lambda$, the smaller the <em>R</em> vector norm and simpler the reward function. But this is a trade-off since you also want to maximize the first term.
The authors claim that there is a phase transition point $\lambda_{o}$,
such that if $\lambda &gt; \lambda_{o}$, <em>R</em>=0 and if $\lambda &lt; \lambda_{o}$, <em>R</em> is bounded away from 0. So the best $\lambda$ would be a value just below $\lambda_{o}$.</p>
<p>So the final optimization objective is as follows.</p>













<figure>


  <a data-fancybox="" href="opti.PNG" data-caption="Final optimization problem for the finite state space case. Source: The paper">
<img src="opti.PNG" alt="" ></a>


  
  
  <figcaption>
    Final optimization problem for the finite state space case. Source: The paper
  </figcaption>


</figure>

<p>Note that the summation term running from 1 to <em>N</em> is so that we maximize across all the states in the MDP. Also note that we use our information of the optimal policy implicitly here since we know the best action <em>a</em><!-- raw HTML omitted -->1<!-- raw HTML omitted --> at every step. This can be solved as a LP problem now.</p>
<h2 id="irl-in-large-state-spaces-using-linear-function-approximation">IRL in large state spaces using linear function approximation</h2>
<p>We now consider a large (possibly infinite) state space. Assume we have <em>n</em> variables in our state space and so think of <em>R</em> now as a function $\Re^n \rightarrow \Re$.</p>
<p>But we don&rsquo;t want to consider all such functions so let us restrict ourselves by only considering functions in the following format. We have used linear function approximation here.</p>













<figure>


  <a data-fancybox="" href="lfa.PNG" data-caption="Linear function approximation for the reward function. Source: The paper">
<img src="lfa.PNG" alt="" ></a>


  
  
  <figcaption>
    Linear function approximation for the reward function. Source: The paper
  </figcaption>


</figure>

<p>The \phi functions are basis functions over the state variables and our job now is to find the $\alpha$ values. Once again, since we&rsquo;re using linear function approximation, we can use LP to solve the problem.</p>
<p>The paper also shows that the value function under a given policy is also a linear function of the $\alpha$ values (refer to the paper to see why this is true). So now, we can rewrite the equation that characterizes our solution set as the following for the large state space case.</p>













<figure>


  <a data-fancybox="" href="gen_char_set.PNG" data-caption="Characterizing the solution set for the large state space case. Source: The paper">
<img src="gen_char_set.PNG" alt="" ></a>


  
  
  <figcaption>
    Characterizing the solution set for the large state space case. Source: The paper
  </figcaption>


</figure>

<p>But we have a problem now. This leads to infinitely many constraints to check because the state space could be infinite and we need to check the condition for each one of those states. Remember we had a summation in the final equation in the last section? That would be an infinite summation in this case. However, the authors circumvent this issue algorithmically by just sampling a large number of states and just checking for those states.</p>
<p>The other issue is that since we have restricted ourselves by using linear function approximation, we may not be able to express all reward functions and hence we&rsquo;ll relax some constraints and pay a penalty when we don&rsquo;t meet the constraints. The final optimization objective is below.</p>













<figure>


  <a data-fancybox="" href="opti2.PNG" data-caption="Final optimization problem for the large state space case. Source: The paper">
<img src="opti2.PNG" alt="" ></a>


  
  
  <figcaption>
    Final optimization problem for the large state space case. Source: The paper
  </figcaption>


</figure>

<p><em>S</em><!-- raw HTML omitted -->o<!-- raw HTML omitted --> is the subsample of states and $p(x) = x$ when $x \geq 0$ and $2x$ when $x &lt; 0$. $\pi$ is the optimal policy. This can be solved with LP now.</p>
<h2 id="irl-from-sampled-trajectories">IRL from sampled trajectories</h2>
<p>Now, we come to the most interesting and most realistic case. We now try to learn from sampled trajectories from the environment. <strong>We do not require an explicit model of the MDP but we do assume the ability to find an optimal policy under any reward function. We also assume the ability to simulate trajectories in the environment with the optimal policy or any other policy we want.</strong> Also assume there is only a single start state <em>s</em><!-- raw HTML omitted -->o<!-- raw HTML omitted -->. This is not a string assumption as if there are several start states with an initial state distribution, add an additional state and connect it to each of them. This is the most realistic case and is different from the previous cases because we don&rsquo;t have the model of the environment i.e. the <em>P</em> matrices.</p>
<p>Once again, <em>R</em> will be expressed as a linear function approximation in the same form as the previous section. Please refer to the paper and convince yourself that it is possible to use Monte Carlo trajectories to estimate a value function that is also linear in the $\alpha$ values. The math is quite simple and straight-forward. This is important because it allows us to use LP again.</p>
<p>So now we have the optimization objective as follows.</p>













<figure>


  <a data-fancybox="" href="opti3.PNG" data-caption="Final optimization problem for the trajectories case. Source: The paper">
<img src="opti3.PNG" alt="" ></a>


  
  
  <figcaption>
    Final optimization problem for the trajectories case. Source: The paper
  </figcaption>


</figure>

<p><em>p</em> is as defined in the previous section. But what is <em>k</em> in the summation? It is the number of policies apart from the optimal policy we are considering at that step of the algorithm. This will be clear in a second. The algorithm is as follows:</p>
<ol>
<li>Start with the optimal policy $\pi^*$ and another random policy $\pi_{1}$. Find the $\alpha$ values that satisfy the above with <em>k</em>=1. Hence find <em>R</em>.</li>
<li>Now using the <em>R</em> we just found, find $\pi_{2}$ that maximizes $V^{\pi_{2}}(s_{o})$. This can be done using some RL algorithm.</li>
<li>Now add $\pi_{2}$ to our current set of policies and optimize the above for $\pi^*$, $\pi_{1}$ and $\pi_{2}$ with <em>k</em>=2.</li>
<li>Keep going until you are &ldquo;satisfied&rdquo;.</li>
</ol>
<p>I will not get into the experiments conducted but I would highly recommend that you read the paper since there are some interesting details and observations.</p>
<h2 id="future-work">Future Work</h2>
<p>The authors plan on finding not just simple reward functions as they have done in this paper i.e. ones with small values but want to do one better to find &ldquo;easy to learn&rdquo; reward functions. I guess this means that &ldquo;simple&rdquo; doesn&rsquo;t always mean &ldquo;easy to learn&rdquo;. They also want a way to be able to account for variation and noise in the state space and action selection process in real world applications. They also hope to find &ldquo;locally consistent&rdquo; reward functions in specific regions of the state space if they find that observed behaviour is far from optimal.</p>

    </div>

    


    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/reinforcement-learning/">Reinforcement Learning</a>
  
</div>



    
      








  
  
    
  
  






  
  
  
    
  
  
  <div class="media author-card">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hub1c2c38cda459ee00afc7beaf7133d62_259593_250x250_fill_q90_lanczos_center.jpeg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://skandavaidyanath.github.io/">Skanda Vaidyanath</a></h5>
      <h6 class="card-subtitle">Second Year Master&rsquo;s Student of Computer Science (AI Track)</h6>
      <p class="card-text">My research interests lie primarily in the area of reinforcement learning (RL) and control to build agents that can acquire complex behaviours in the real world via interaction.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:svaidyan@stanford.edu" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://in.linkedin.com/in/skanda-vaidyanath" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/skandavaidyanath" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/skanda_vaid" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="/files/resume.pdf" >
        <i class="ai ai-cv"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/post/modeling-rl-problems/">Modeling RL Problems</a></li>
          
          <li><a href="/talk/usc-ict/">The Human-Swarm Project</a></li>
          
          <li><a href="/post/bridging-the-gaps-with-rl/">Bridging the Gaps With Reinforcement Learning</a></li>
          
          <li><a href="/project/mpii/">Deep RL for IR applications</a></li>
          
          <li><a href="/project/usc-ict/">The Human Swarm Project</a></li>
          
        </ul>
      </div>
      
    

    

    


  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.c5b98a8914247ae95edfbe976665d60f.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
