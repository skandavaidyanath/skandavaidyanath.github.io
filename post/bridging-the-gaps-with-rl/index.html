<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.5.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Skanda Vaidyanath">

  
  
  
    
  
  <meta name="description" content="In this post, I will be talking about a unique way to use reinforcement learning (RL) in deep learning applications. I definitely recommend brushing up some deep learning fundamentals and if possible, some policy gradient fundamentals as well before you get started with this post.
Traditionally, RL is used to solve sequential decision making problems in the video game space or robotics space or any other space where there is a concrete RL task at hand.">

  
  <link rel="alternate" hreflang="en-us" href="https://skandavaidyanath.github.io/post/bridging-the-gaps-with-rl/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.8565976d7831ba0a0daf18ae5d4ff0e8.css">

  

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://skandavaidyanath.github.io/post/bridging-the-gaps-with-rl/">

  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Skanda Vaidyanath">
  <meta property="og:url" content="https://skandavaidyanath.github.io/post/bridging-the-gaps-with-rl/">
  <meta property="og:title" content="Bridging the Gaps With Reinforcement Learning | Skanda Vaidyanath">
  <meta property="og:description" content="In this post, I will be talking about a unique way to use reinforcement learning (RL) in deep learning applications. I definitely recommend brushing up some deep learning fundamentals and if possible, some policy gradient fundamentals as well before you get started with this post.
Traditionally, RL is used to solve sequential decision making problems in the video game space or robotics space or any other space where there is a concrete RL task at hand."><meta property="og:image" content="https://skandavaidyanath.github.io/post/bridging-the-gaps-with-rl/featured.jpg">
  <meta property="twitter:image" content="https://skandavaidyanath.github.io/post/bridging-the-gaps-with-rl/featured.jpg"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-11-04T03:10:09&#43;05:30">
    
    <meta property="article:modified_time" content="2019-11-04T03:10:09&#43;05:30">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://skandavaidyanath.github.io/post/bridging-the-gaps-with-rl/"
  },
  "headline": "Bridging the Gaps With Reinforcement Learning",
  
  "image": [
    "https://skandavaidyanath.github.io/post/bridging-the-gaps-with-rl/featured.jpg"
  ],
  
  "datePublished": "2019-11-04T03:10:09+05:30",
  "dateModified": "2019-11-04T03:10:09+05:30",
  
  "author": {
    "@type": "Person",
    "name": "Skanda Vaidyanath"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Skanda Vaidyanath",
    "logo": {
      "@type": "ImageObject",
      "url": "https://skandavaidyanath.github.io/img/icon-512.png"
    }
  },
  "description": "In this post, I will be talking about a unique way to use reinforcement learning (RL) in deep learning applications. I definitely recommend brushing up some deep learning fundamentals and if possible, some policy gradient fundamentals as well before you get started with this post.\nTraditionally, RL is used to solve sequential decision making problems in the video game space or robotics space or any other space where there is a concrete RL task at hand."
}
</script>

  

  


  


  





  <title>Bridging the Gaps With Reinforcement Learning | Skanda Vaidyanath</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Skanda Vaidyanath</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#post"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/courses/"><span>Courses</span></a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article">

  




















  
  
    
  


<div class="article-container pt-3">
  <h1>Bridging the Gaps With Reinforcement Learning</h1>

  

  



<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/admin/">Skanda Vaidyanath</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Nov 4, 2019
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    7 min read
  </span>
  

  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/categories/reinforcement-learning/">Reinforcement Learning</a>, <a href="/categories/deep-learning/">Deep Learning</a></span>
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://skandavaidyanath.github.io/post/bridging-the-gaps-with-rl/&amp;text=Bridging%20the%20Gaps%20With%20Reinforcement%20Learning" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://skandavaidyanath.github.io/post/bridging-the-gaps-with-rl/&amp;t=Bridging%20the%20Gaps%20With%20Reinforcement%20Learning" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Bridging%20the%20Gaps%20With%20Reinforcement%20Learning&amp;body=https://skandavaidyanath.github.io/post/bridging-the-gaps-with-rl/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://skandavaidyanath.github.io/post/bridging-the-gaps-with-rl/&amp;title=Bridging%20the%20Gaps%20With%20Reinforcement%20Learning" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Bridging%20the%20Gaps%20With%20Reinforcement%20Learning%20https://skandavaidyanath.github.io/post/bridging-the-gaps-with-rl/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://skandavaidyanath.github.io/post/bridging-the-gaps-with-rl/&amp;title=Bridging%20the%20Gaps%20With%20Reinforcement%20Learning" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>


  

</div>

  














</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 720px; max-height: 540px;">
  <div style="position: relative">
    <img src="/post/bridging-the-gaps-with-rl/featured_hu11790c4b9d7964d5abb0be2839a6821f_1316322_720x0_resize_q90_lanczos.jpg" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <p>In this post, I will be talking about a unique way to use reinforcement learning (RL) in deep learning applications. I definitely recommend brushing up some deep learning fundamentals and if possible, some policy gradient fundamentals as well before you get started with this post.</p>
<p>Traditionally, RL is used to solve sequential decision making problems in the video game space or robotics space or any other space where there is a concrete RL task at hand. There are several applications in the fields of video games and robotics where the task at hand can be very easily seen as an RL problem and can be modeled appropriately as well. But RL as a technique is quite versatile and can be used in several other domains to train neural networks that are traditionally trained in a supervised fashion. We&rsquo;ll talk about one very important such application in this post. Along the way, I&rsquo;ll also try to convince you that this isn&rsquo;t really a different way to use RL but rather just a different way to look at the traditional RL problem. So with that, lets begin!</p>
<h2 id="non-differentiable-steps-in-deep-learning-the-gaps">Non-differentiable steps in deep learning: The Gaps</h2>
<p>Sometimes when we&rsquo;re coming up with neural network architectures, we may need to incorporate some non-differentiable operations as a part of our network. Now this is a problem as we can&rsquo;t backpropagate losses through such operation and hence lets call these &ldquo;gaps&rdquo;. So what are some common gaps we come across in neural networks?</p>
<p>On a side-note, before we start talking about some &ldquo;real gaps&rdquo;, its worth mentioning that the famous ReLU function is a non-differentiable function but we overcome that gap by setting the derivative at 0 to either 1 or 0 and get away with it.</p>
<p>Now lets take a better example &ndash; variational autoencoders (VAE). Without going into two many details, the VAE network outputs two vectors: a $\mu$ vector and a $\sigma$ vector and it involves a crucial sampling step where we sample from the distribution <em>N($\mu$, $\sigma$)</em> as a part of the network. Now sampling is a gap as it is a non-differentiable step. You should stop here and convince yourself that this is, in fact true. When we sample, we don&rsquo;t know what the outcome will be and hence the function in not differentiable. So how do they get over this in the VAE case? They use a clever trick.
Instead of sampling from <em>N</em>($\mu$, $\sigma$), they just rewrite this as $\mu$ + $\sigma$<em>N(0,1)</em> where they sample from the standard normal function. This neat trick now makes the expression differentiable because we just need the $\mu$ and $\sigma$ quantities to be differentiable and we don&rsquo;t care about the <em>N(0,1)</em>. Remember that we only need to differentiate with respect to the parameters of our network (brush up some backpropagation basics if you&rsquo;re confused here) and hence we need to differentiate only with respect to $\mu$ and $\sigma$ and not the standard normal distribution. For more details about VAEs read <a href="https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf">this post</a> or <a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73">this one</a>.</p>













<figure>


  <a data-fancybox="" href="vae.PNG" data-caption="Variational Autoencoders. Source: here">
<img src="vae.PNG" alt="" ></a>


  
  
  <figcaption>
    Variational Autoencoders. Source: <a href="https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf">here</a>
  </figcaption>


</figure>

<p>So as it turned out, that wasn&rsquo;t a very good example either but we&rsquo;re starting to understand what we mean by gaps now and how common they are. Some common examples of gaps in networks are sampling operations and the argmax operation. Once again, convince yourself that argmax is not a differentiable function. Assume you have a function that takes argmax of two quantities <em>(x1,x2)</em>. When <em>x1</em> &gt; <em>x2</em>, this has value 0 (zero-indexed) and when <em>x1</em> &lt; <em>x2</em>, it has value 1. Say the function is not defined on the <em>x1==x2</em> line or define it as you wish (0 or 1). Now if you can visualise the 2D plane, you&rsquo;ll see that the function is not differentiable as we move across the <em>x1==x2</em> line. So argmax isn&rsquo;t differentiable <em>but max is a differentiable function</em> (recall max pooling in CNNs). Read <a href="https://www.reddit.com/r/MachineLearning/comments/4e2get/argmax_differentiable/">this thread</a> for more details.</p>
<p>These functions are commonly used in natural language processing (NLP) applications, information retrieval (IR) applications and Computer Vision (CV) applications as well. For example, a sampling function could be used to select words from a sentence based on a probability distribution in an NLP application or an argmax function could be used to find the highest ranked document in an IR application. <a href="https://jhui.github.io/2017/03/15/Soft-and-hard-attention/">Hard attention</a> uses sampling techniques which involves non-differentiable computation.</p>
<p>So its quite clear that these gaps are common in several deep learning architectures and sometimes, it could even be useful to introduce such a gap in the network intentionally to reap added benefits. The only question is, <em>how do we bridge these gaps?</em></p>
<h2 id="reinforcement-learning-and-policy-gradients-the-bridge">Reinforcement Learning and Policy Gradients: The Bridge</h2>
<p>Policy gradients are a class of algorithms in RL. There are several policy gradient algorithms and <a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html">this</a> is a great blog that lists out almost all of them. But without going into too many details, these algorithms work in the policy space by updating the parameters of the policy we&rsquo;re trying to learn. That means we don&rsquo;t necessarily need to find the value function of different states but we can directly alter our policy until we&rsquo;re happy.
The most common policy gradient (PG) algorithm is the REINFORCE which is a Monte Carlo algorithm. This means we run an entire episode and make changes to our policy only at the end of each episode and not at every step. We make these changes based on the returns that we received by taking a given action from a given state in the episode. I skip the derivation of the policy gradient here but it can be found in the link above. The final result is in the image below.</p>













<figure>


  <a data-fancybox="" href="pg.PNG" data-caption="The Policy Gradient. Source: here">
<img src="pg.PNG" alt="" ></a>


  
  
  <figcaption>
    The Policy Gradient. Source: <a href="https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63">here</a>
  </figcaption>


</figure>

<p>The key idea here is that in policy gradient methods, we are allowed to <em>sample different actions from a given state and wait till the end of an episode before we make updates to our network</em>. So if we have a sampling operation as a part of our network, we can introduce a policy gradient and think of it as sampling actions in a given state in an RL framework. A similar procedure can also be followed if we had argmax in place of the sampling operation.</p>
<p>Consider a neural network now with a gap. The images below are taken from <a href="http://karpathy.github.io/2016/05/31/rl/">this blog</a> on Policy Gradients written by Andrej Karpathy.</p>
<p>












<figure>


  <a data-fancybox="" href="karpathy1.PNG" data-caption="Gaps in a neural network. Source: Karpathy&rsquo;s blog">
<img src="karpathy1.PNG" alt="" ></a>


  
  
  <figcaption>
    Gaps in a neural network. Source: Karpathy&rsquo;s <a href="http://karpathy.github.io/2016/05/31/rl/">blog</a>
  </figcaption>


</figure>














<figure>


  <a data-fancybox="" href="karpathy2.PNG" data-caption="The sampling operation. Source: Karpathy&rsquo;s blog">
<img src="karpathy2.PNG" alt="" ></a>


  
  
  <figcaption>
    The sampling operation. Source: Karpathy&rsquo;s <a href="http://karpathy.github.io/2016/05/31/rl/">blog</a>
  </figcaption>


</figure>
</p>
<p>So now we can train the blue arrows i.e. the differentiable path as usual. But to train the red arrows, we need to introduce a policy gradient and as we sample, we ensure with the help of the policy gradient that we encourage samples that led to a lower loss. In this way, we are &ldquo;training&rdquo; the sampling operation or one could say, propagating the loss through the sampling operation! Note that the updates to the red arrows happen independently than those of the blue arrows.
Note that in the diagrams above, there isn&rsquo;t really a gap per-say because the blue arrows go all the way from start to finish. So there is a differentiable path and a non-differentiable path. A true gap would mean there would be no completely differentiable path at all. In this case, we need to make sure that the loss functions on either side of the gap are &ldquo;in sync&rdquo; and are being optimized in such a way that it facilitates joint training and achieves a common goal. This is often not as easy as it sounds.</p>
<p>I said at the start that as obscure as it seems, this is still the traditional RL problem we&rsquo;re used to with the MDP and states and actions. We can still look at this entire setup as a traditional RL problem if we think of the inputs to the neural network as the state and the sampling process as sampling different actions from that given state. Now what is the reward function? This depends on what comes after the gap and could be an output from the rest of the network or it could be a completely independent reward function that you came up with as well. So at the end of the day, it is still the same MDP with the traditional setup but just used in a very different way.</p>

    </div>

    


    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/reinforcement-learning/">Reinforcement Learning</a>
  
  <a class="badge badge-light" href="/tags/deep-learning/">Deep Learning</a>
  
</div>



    
      








  
  
    
  
  






  
  
  
    
  
  
  <div class="media author-card">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hub1c2c38cda459ee00afc7beaf7133d62_259593_250x250_fill_q90_lanczos_center.jpeg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://skandavaidyanath.github.io/">Skanda Vaidyanath</a></h5>
      <h6 class="card-subtitle">Second Year Master&rsquo;s Student of Computer Science (AI Track)</h6>
      <p class="card-text">My research interests lie primarily in the area of reinforcement learning (RL) and control to build agents that can acquire complex behaviours in the real world via interaction.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:svaidyan@stanford.edu" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://in.linkedin.com/in/skanda-vaidyanath" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/skandavaidyanath" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/skanda_vaid" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="/files/resume.pdf" >
        <i class="ai ai-cv"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/project/mpii/">Deep RL for IR applications</a></li>
          
          <li><a href="/project/usc-ict/">The Human Swarm Project</a></li>
          
          <li><a href="/project/cqa/">Community Question Answering for a distance-learning platform</a></li>
          
          <li><a href="/project/mitacs/">Personalized Learning from Job Descriptions</a></li>
          
          <li><a href="/project/igcar/">Search Engine On a Nuclear Corpus</a></li>
          
        </ul>
      </div>
      
    

    

    


  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.c5b98a8914247ae95edfbe976665d60f.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
