<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.5.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Skanda Vaidyanath">

  
  
  
    
  
  <meta name="description" content="Disclaimer: The content for this article does not come from any textbook or other reliable sources. They are observations made purely from my very limited experience with RL.
I recommend that you gather some RL basics before you proceed to read this article. The first couple of posts from the course on my page could be a good start.
In this article, I&rsquo;m going to talk about something that I haven&rsquo;t seen anywhere before and nobody really talks about it but I&rsquo;m going to take a shot at it.">

  
  <link rel="alternate" hreflang="en-us" href="https://skandavaidyanath.github.io/post/modeling-rl-problems/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.8565976d7831ba0a0daf18ae5d4ff0e8.css">

  

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://skandavaidyanath.github.io/post/modeling-rl-problems/">

  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Skanda Vaidyanath">
  <meta property="og:url" content="https://skandavaidyanath.github.io/post/modeling-rl-problems/">
  <meta property="og:title" content="Modeling RL Problems | Skanda Vaidyanath">
  <meta property="og:description" content="Disclaimer: The content for this article does not come from any textbook or other reliable sources. They are observations made purely from my very limited experience with RL.
I recommend that you gather some RL basics before you proceed to read this article. The first couple of posts from the course on my page could be a good start.
In this article, I&rsquo;m going to talk about something that I haven&rsquo;t seen anywhere before and nobody really talks about it but I&rsquo;m going to take a shot at it."><meta property="og:image" content="https://skandavaidyanath.github.io/post/modeling-rl-problems/featured.jpg">
  <meta property="twitter:image" content="https://skandavaidyanath.github.io/post/modeling-rl-problems/featured.jpg"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-01-16T22:12:05&#43;05:30">
    
    <meta property="article:modified_time" content="2020-01-16T22:12:05&#43;05:30">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://skandavaidyanath.github.io/post/modeling-rl-problems/"
  },
  "headline": "Modeling RL Problems",
  
  "image": [
    "https://skandavaidyanath.github.io/post/modeling-rl-problems/featured.jpg"
  ],
  
  "datePublished": "2020-01-16T22:12:05+05:30",
  "dateModified": "2020-01-16T22:12:05+05:30",
  
  "author": {
    "@type": "Person",
    "name": "Skanda Vaidyanath"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Skanda Vaidyanath",
    "logo": {
      "@type": "ImageObject",
      "url": "https://skandavaidyanath.github.io/img/icon-512.png"
    }
  },
  "description": "Disclaimer: The content for this article does not come from any textbook or other reliable sources. They are observations made purely from my very limited experience with RL.\nI recommend that you gather some RL basics before you proceed to read this article. The first couple of posts from the course on my page could be a good start.\nIn this article, I\u0026rsquo;m going to talk about something that I haven\u0026rsquo;t seen anywhere before and nobody really talks about it but I\u0026rsquo;m going to take a shot at it."
}
</script>

  

  


  


  





  <title>Modeling RL Problems | Skanda Vaidyanath</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Skanda Vaidyanath</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#post"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/courses/"><span>Courses</span></a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article">

  




















  
  
    
  


<div class="article-container pt-3">
  <h1>Modeling RL Problems</h1>

  

  



<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/admin/">Skanda Vaidyanath</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Jan 16, 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    9 min read
  </span>
  

  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/categories/reinforcement-learning/">Reinforcement Learning</a></span>
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://skandavaidyanath.github.io/post/modeling-rl-problems/&amp;text=Modeling%20RL%20Problems" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://skandavaidyanath.github.io/post/modeling-rl-problems/&amp;t=Modeling%20RL%20Problems" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Modeling%20RL%20Problems&amp;body=https://skandavaidyanath.github.io/post/modeling-rl-problems/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://skandavaidyanath.github.io/post/modeling-rl-problems/&amp;title=Modeling%20RL%20Problems" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Modeling%20RL%20Problems%20https://skandavaidyanath.github.io/post/modeling-rl-problems/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://skandavaidyanath.github.io/post/modeling-rl-problems/&amp;title=Modeling%20RL%20Problems" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>


  

</div>

  














</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 720px; max-height: 405px;">
  <div style="position: relative">
    <img src="/post/modeling-rl-problems/featured_hue826dd50edae406767403690a51c4df1_142388_720x0_resize_q90_lanczos.jpg" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <p><strong>Disclaimer: The content for this article does not come from any textbook or other reliable sources. They are observations made purely from my very limited experience with RL.</strong></p>
<p>I recommend that you gather some RL basics before you proceed to read this article. The first couple of posts from <a href="https://skandavaidyanath.github.io/courses/rl-course/">the course</a> on my page could be a good start.</p>
<p>In this article, I&rsquo;m going to talk about something that I haven&rsquo;t seen anywhere before and nobody really talks about it but I&rsquo;m going to take a shot at it. I&rsquo;ve learned RL on my own and hence I&rsquo;ve read several articles on the internet about RL but most of them are about the different algorithms starting from the most basic dynamic programming and going on till the most complex Soft Actor Critic. But having spent some time conducting RL research, I find that nobody really talks about, according to me, the most challenging and interesting problem in RL &ndash; the actual <em>modeling of the problem</em>.</p>
<p>So what do we mean by &ldquo;modeling the problem&rdquo;? If someone told you to solve the problem of self-driving cars using RL, how would you start? Lets say you decide to go ahead with some MDP framework, what would your states be? What would your actions be? What reward signal would you use? Would you solve the entire &ldquo;self-driving&rdquo; problem at once or would you want to break it down into smaller components? This is what I mean by &ldquo;modeling a problem&rdquo; and in my experience, has turned out to be the most important and challenging part of the bigger problem. Its something that sounds so simple but these early decisions are so important and I strongly believe that solid modeling could lead to better performance than using complex algorithms on weakly structured problems.</p>
<p>This is in fact one of the things that drew me to RL because I thought modeling problems was extremely interesting and challenging as well. I also couldn&rsquo;t find anything quite similar to this in other domains. Feature selection in supervised learning comes close but there are specific techniques and tests you can conduct to come up with the best set of features given a large feature set. So this seemed like a unique problem specific to the RL world. But if it is such an exciting problem why hasn&rsquo;t anyone written about it? My guess is that there is no simple answer to the question of &ldquo;How to model an RL problem?&rdquo;. The answer is that &ldquo;It depends on the problem&rdquo;. But I do believe there are some very simple guidelines you can follow or rather questions you can ask yourself when you&rsquo;re modeling your problem. So here we go! Five questions that will help you model RL problems. There&rsquo;s one additional question I guess that I&rsquo;m implicitly answering; we will stick to the MDP framework and not talk about SMDPs or POMDPs for now (if you&rsquo;ve never heard of these, good).</p>
<ol>
<li>What are the actions I need?</li>
<li>Are my actions instantaneous?</li>
<li>What is my state space and <em>can I make it smaller</em>?</li>
<li>How complex is my problem and can I split it into smaller and easier problems?</li>
<li>What is the simplest reward function I can use?</li>
</ol>
<p>So let&rsquo;s go through them one by one.</p>
<h2 id="what-are-the-actions-i-need">What are the actions I need?</h2>
<p>This is the first question you need to ask yourself when you&rsquo;re handed a problem. What is the agent going to achieve and what actions will it need to achieve that task? This decision is often the easiest to make because this comes along with the problem description. So if the task is to navigate a car from one position of a square grid to another, the directions of movement would probably be the most natural choice for actions. Although this is much harder to see in a problem like Chess or self-driving cars, with a little bit of effort, we can think about all the different actions we have at our disposal. The important thing to keep track of however, is whether the action space is continuous or discrete and if discrete, how many actions you have. This information could be useful to decide if you maybe want to split the problem up into smaller easier problems.
<em>Remember, the more actions you have, the harder the task is.</em></p>













<figure>


  <a data-fancybox="" href="taxi.png" data-caption="The Taxi Problem. Source: Google Images">
<img src="taxi.png" alt="" ></a>


  
  
  <figcaption>
    The Taxi Problem. Source: Google Images
  </figcaption>


</figure>

<h2 id="are-my-actions-instantaneous">Are my actions instantaneous?</h2>
<p>Once you&rsquo;ve figured out what your actions are, we can take a closer look at them now. One thing we want to look for is whether the actions are instantaneous or not. Sometimes there could be actions that realistically take time to execute. For example, in the above car-grid problem, if your actions were &ldquo;move the car to the green square&rdquo;, this action could take time to execute realistically. Modeling them as
aneous actions wouldn&rsquo;t be accurate. In the MDP framework, we need all actions to be instantaneous. So what do we do when we have actions that take time? First, try to get rid of them or replace them with simpler instantaneous actions. But this is a trade-off because this could increase the number of actions. Next, think about whether the &ldquo;long&rdquo; action could be a separate problem on its own and that could be modeled as a separate smaller RL problem. But this starts getting into hierarchical RL and I wouldn&rsquo;t get into it unless you&rsquo;re sure about what I&rsquo;m doing.
One thing that I&rsquo;ve done in the past to model actions that take time (that could even vary, every time the action is played) is to model time using probabilities and put a variable in the state space (we&rsquo;ll get to this) indicating that the current action is in progress. So where do the probabilities come into play? Lets say a &ldquo;long&rdquo; action is in progress. Now we keep playing an extra &ldquo;WAIT&rdquo; action or &ldquo;NOP&rdquo; action and flip a coin (not a fair one) and if its heads, the &ldquo;long&rdquo; action ends. This seems convoluted but I&rsquo;ve found that sometimes it could make life a lot simpler by introducing not too much complexity into the state space or action space. Otherwise, simply foraying into SMDPs and hierarchical RL is fine as well if you&rsquo;re confident.</p>
<h2 id="what-is-my-state-space-and-can-i-make-it-smaller">What is my state space and can I make it smaller?</h2>
<p>So what variables are important for my problem and what goes into my state space? The crucial step here is to think about your actions. What variables do you need to decide what action you want to play in each state? Even thinking about what information a human might need could be useful.
The key is to be minimalistic. Use as few variables as possible and use variables that don&rsquo;t take too many values. Try to maintain a small, discrete state space. This is almost always never possible but its definitely worth the shot. If you can model a problem that you can solve with one of the basic RL algorithms without getting into function approximation (for the uninitiated, think of it as using deep learning), there&rsquo;s nothing like it. But this is difficult and one of the hardest parts of the modeling problem.
It takes a long time at the start of your project and it could get frustrating because you&rsquo;re working with a white board and a marker and not a keyboard and a monitor. But trust me, the effort will be worth it. If you can design a small, simple state space then half the battle is won. At USC, when I was modeling my problem, it took over two weeks but we reduced our state space from over 33 billion to just 1440!</p>
<h2 id="how-complex-is-my-problem-and-can-i-split-it-into-smaller-and-easier-problems">How complex is my problem and can I split it into smaller and easier problems?</h2>
<p>I&rsquo;m afraid there is no easy way to answer this that applies to all RL problems. But let me give you an example that might help. If I&rsquo;m trying to make a robot learn to play tennis, I might want to split the big problem into smaller problems of &ldquo;learning how to serve&rdquo; or &ldquo;learning how to play a forehand&rdquo;, etc. One way of figuring out whether your problem needs breaking down is to check if all the actions make sense with each other i.e. can you play all actions at almost all states? Do you need all the state variables to make a call on whether you want to play an action or can I judge some actions just based on a subset of state variables?
These questions may help you partition your state space and action space into multiple simpler problems that will make it easy to learn. Its not always easy to find these patterns and partitions though. And you also need to make sure there is an easy way to put these sub-problems back together. Nevertheless, if your problem is too complex i.e. too many states and/or too many actions then maybe it is worth spending the time solving multiple problems.</p>
<h2 id="what-is-the-simplest-reward-function-i-can-use">What is the simplest reward function I can use?</h2>
<p>And finally, my favourite question. Questions 3 and 5 are by far the hardest of the lot and this question is probably the hardest of the lot. Which makes writing the answer so much easier. Researchers have recognized the difficulty of crafting reward functions and created a new field called Inverse Reinforcement Learning just to address the issue. But once again, there are some guidelines you can follow.
Usually, I try to stick to the simplest reward function possible. What do I mean by simplest? Small values and as sparse as possible. So if I&rsquo;m designing a reward function for chess, my rewards would only be at the end of the game and would be a +1, 0, -1 for a win, draw and loss respectively. Another reward function could be for every move played but this would be harder to craft and there&rsquo;s no reason for you to try this until you know the simple reward function doesn&rsquo;t work. But in the problem of chess, it is qite straight-forward but there are other problems where finding an elegant reward function might not be as simple.</p>
<p>So there you have it. I hope that helped and hopefully when you model your next problem, these tips will help. But thinking about all of this modeling has made me think about inverse reinforcement learning and meta learning a lot more. Is it possible for RL models to learn the optimal design of a problem? Not just the reward function but also the state space and the action space? And if it can, can it learn some common properties that we can learn across all RL problems? These are tough questions that I hope to answer some day.</p>

    </div>

    


    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/reinforcement-learning/">Reinforcement Learning</a>
  
</div>



    
      








  
  
    
  
  






  
  
  
    
  
  
  <div class="media author-card">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hub1c2c38cda459ee00afc7beaf7133d62_259593_250x250_fill_q90_lanczos_center.jpeg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://skandavaidyanath.github.io/">Skanda Vaidyanath</a></h5>
      <h6 class="card-subtitle">Second Year Master&rsquo;s Student of Computer Science (AI Track)</h6>
      <p class="card-text">My research interests lie primarily in the area of reinforcement learning (RL) and control to build agents that can acquire complex behaviours in the real world via interaction.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:svaidyan@stanford.edu" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://in.linkedin.com/in/skanda-vaidyanath" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/skandavaidyanath" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/skanda_vaid" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="/files/resume.pdf" >
        <i class="ai ai-cv"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/talk/usc-ict/">The Human-Swarm Project</a></li>
          
          <li><a href="/post/bridging-the-gaps-with-rl/">Bridging the Gaps With Reinforcement Learning</a></li>
          
          <li><a href="/project/mpii/">Deep RL for IR applications</a></li>
          
          <li><a href="/project/usc-ict/">The Human Swarm Project</a></li>
          
        </ul>
      </div>
      
    

    

    


  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.c5b98a8914247ae95edfbe976665d60f.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
