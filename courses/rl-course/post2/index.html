<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.5.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Skanda Vaidyanath">

  
  
  
    
  
  <meta name="description" content="In this post, we&rsquo;ll try to get into the real nitty-gritties of RL and build on the intuition that we gained from the last article. So we&rsquo;ll bring in some mathematical foundation and then introduce some RL parlance that we will use for the rest of this course. I&rsquo;ll stick to the standard notation and jargon from the RL book.
Three Challenges Before we begin with the mathematical foundations of RL, I&rsquo;d like to point out three challenges in RL we need to account for if we were to come up with RL algorithms of our own.">

  
  <link rel="alternate" hreflang="en-us" href="https://skandavaidyanath.github.io/courses/rl-course/post2/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.8565976d7831ba0a0daf18ae5d4ff0e8.css">

  

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://skandavaidyanath.github.io/courses/rl-course/post2/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Skanda Vaidyanath">
  <meta property="og:url" content="https://skandavaidyanath.github.io/courses/rl-course/post2/">
  <meta property="og:title" content="RL Fundamentals and MDPs | Skanda Vaidyanath">
  <meta property="og:description" content="In this post, we&rsquo;ll try to get into the real nitty-gritties of RL and build on the intuition that we gained from the last article. So we&rsquo;ll bring in some mathematical foundation and then introduce some RL parlance that we will use for the rest of this course. I&rsquo;ll stick to the standard notation and jargon from the RL book.
Three Challenges Before we begin with the mathematical foundations of RL, I&rsquo;d like to point out three challenges in RL we need to account for if we were to come up with RL algorithms of our own."><meta property="og:image" content="https://skandavaidyanath.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://skandavaidyanath.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-05-05T00:00:00&#43;01:00">
    
    <meta property="article:modified_time" content="2019-05-05T00:00:00&#43;01:00">
  

  



  


  


  





  <title>RL Fundamentals and MDPs | Skanda Vaidyanath</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Skanda Vaidyanath</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#post"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/courses/"><span>Courses</span></a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  

<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      





<form class="docs-search d-flex align-items-center">
  <button class="btn docs-toggle d-md-none p-0 mr-3" type="button" data-toggle="collapse" data-target="#docs-nav" aria-controls="docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
    <span><i class="fas fa-bars"></i></span>
  </button>

  
  <input name="q" type="search" class="form-control" placeholder="Search..." autocomplete="off">
  
</form>

<nav class="collapse docs-links" id="docs-nav">
  
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/courses/rl-course/">Overview</a>

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/courses/rl-course/post1/">Reinforcement Learning from Scratch</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/courses/rl-course/post1/">Introduction: Why RL?</a>
      </li>
      
      <li class="active">
        <a href="/courses/rl-course/post2/">RL Fundamentals and MDPs</a>
      </li>
      
      <li >
        <a href="/courses/rl-course/post3/">Policies, Value Functions and the Bellman Equation</a>
      </li>
      
    </ul>
    

  </div>
  
  
</nav>

    </div>

    
    <div class="d-none d-xl-block col-xl-2 docs-toc">
      <ul class="nav toc-top">
        <li><a href="#" id="back_to_top" class="docs-toc-title">Contents</a></li>
      </ul>

      <nav id="TableOfContents">
  <ul>
    <li><a href="#three-challenges">Three Challenges</a></li>
    <li><a href="#markov-decision-processes">Markov Decision Processes</a>
      <ul>
        <li><a href="#references">References</a></li>
      </ul>
    </li>
  </ul>
</nav>

      
    </div>
    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">

      <article class="article">

        <div class="docs-article-container">
          <h1>RL Fundamentals and MDPs</h1>

          <div class="article-style">
            <p>In this post, we&rsquo;ll try to get into the real nitty-gritties of RL and build on the intuition that we gained from the <a href="https://skandavaidyanath.github.io/courses/rl-course/post1/">last article</a>. So we&rsquo;ll bring in some mathematical foundation and then introduce some RL parlance that we will use for the rest of this course. I&rsquo;ll stick to the standard notation and jargon from the RL book.</p>
<h2 id="three-challenges">Three Challenges</h2>
<p>Before we begin with the mathematical foundations of RL, I&rsquo;d like to point out three challenges in RL we need to account for if we were to come up with RL algorithms of our own. Once again, I&rsquo;m going to move on to a new example so lets take chess this time.</p>



  











<figure>


  <a data-fancybox="" href="/img/post1_alphazero.jpg" data-caption="AlphaZero: Chess. Source: Google Images">
<img src="/img/post1_alphazero.jpg" alt="" ></a>


  
  
  <figcaption>
    AlphaZero: Chess. Source: Google Images
  </figcaption>


</figure>

<p>So we want to teach our agent (recall what this means from the previous post) how to play the game of chess. Lets assume we have some sort of reward function in place where we get some small positive rewards for capturing a piece and small negative rewards for losing a piece depending on the importance of the piece (so losing a queen would lead to a negative reward of larger magnitude than losing a pawn). We also have some large positive final reward for winning the game and a large positive negative reward for losing. If you&rsquo;re wondering whether just this large final reward is a sufficient reward function on its own, you&rsquo;re probably right and it probably is, but lets stick to this for the sake of illustration.</p>
<p>Now assume we have an RL algorithm that can look at several games of chess and the rewards and learn to play chess on its own. What would this algorithm need to account for? We spoke about trial and error being the basis of any RL algorithm in the previous post that is exactly what our algorithm would do as well. It starts playing random moves and when it plays a good move (positive reward), it remembers to play that move the next time it is in a similar situation. This seems fine on the face of it, but there is an issue. Maybe the algorithm found a good move to play at a given position, <em>but what if there was a better move?</em></p>
<p>We need some way for the algorithm to account for the possibility of there being a better move than the one it has found already. So when we train our agent we need to make sure the agent doesn&rsquo;t greedily play the best move it knows all the time but also plays some different moves, hoping that they may be better than the one it already found. This is called the <em>exploration-exploitation tradeoff</em> in RL. Usually, RL algorithms tend to explore i.e. play many random moves initially and when the agent is more sure about the best moves under different circumstances, it starts exploiting that knowledge. We will revisit this problem in the next post with another example.</p>
<p>Lets move on to the next challenge that our RL algorithm will have to account for. Lets say our RL algorithm is learning from a game of chess again where the player sacrifices the queen but goes on to win the game. The RL agent immediately registers a negative reward for the loss of the queen but the large positive reward for winning the game only comes much later. But it is entirely possible that the very queen sacrifice that the RL agent probably classified as a bad move, was the reason for the player winning the game. So it is possible that a move seems like a &ldquo;bad&rdquo; one in the short term but in the long term, could be a very &ldquo;good&rdquo; move. How do we account for this in our algorithm? This is the concept of <em>delayed rewards</em> and we will deal with a simple yet elegant solution for this as well as we go through this post. We will look at another example for this as well in the next post.</p>
<p>And finally our RL algorithm should learn to generalize i.e. it should be able to efficiently learn from the data it collects. One of the major drawbacks in RL is that it requires a lot of data to learn so <em>generalization</em> is important for RL algorithms.</p>
<p>Note that RL is based on the <em>reward hypothesis</em> that states that:</p>
<blockquote>
<p>All goals can be described by the maximisation of expected cumulative reward</p>
</blockquote>
<p>Think about whether you agree with this statement.</p>
<p>With that background, lets talk about how RL problems are modeled and get into some math.</p>
<h2 id="markov-decision-processes">Markov Decision Processes</h2>
<blockquote>
<p>The future is independent of the past, given the present</p>
</blockquote>
<p>


  











<figure>


  <a data-fancybox="" href="/img/post2_mdp_example.jpg" data-caption="A Markov Decision Process. Source: here">
<img src="/img/post2_mdp_example.jpg" alt="" ></a>


  
  
  <figcaption>
    A Markov Decision Process. Source: <a href="https://randomant.net/reinforcement-learning-concepts/">here</a>
  </figcaption>


</figure>




  











<figure>


  <a data-fancybox="" href="/img/post2_rl_with_not.jpg" data-caption="The RL Framework (with some additional details). Source: Google Images">
<img src="/img/post2_rl_with_not.jpg" alt="" ></a>


  
  
  <figcaption>
    The RL Framework (with some additional details). Source: Google Images
  </figcaption>


</figure>
</p>
<p>Almost all RL problems can be modeled as a Markov Decision Process (MDP). So what is an MDP? An MDP is a mathematical model that we will be seeing a lot over this course. Most of our RL environments, will be MDPs. An MDP can be defined as a five tuple:
$$\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$$</p>
<p>Lets take a closer look at what all of this means. Note that in this section, I do not follow the notation in the RL book and follow the notation <a href="https://ai.stanford.edu/~ang/papers/icml00-irl.pdf">here</a> which is quite commonly followed.</p>
<ul>
<li><em>S</em> : This is the set of states of the MDP. In an RL setting, this would correspond to different settings of the environment i.e. each setting of the environment is a state in the MDP. In the previous post, we spoke about how RL was all about choosing the right actions at the right times i.e. depending on the state of the environment. This is the state we were talking about. A state in chess or tic-tac-toe could be the board positions or while riding a bike could be some combination of the pertinent variables like the angle of the bike with thr ground, the wind speed, etc. The <em>S</em> variable represents the set of all unique states in the MDP.</li>
<li><em>A</em> : This is the set of all actions of the MDP. We already spoke about actions briefly. Actions describe the possible moves in a game of chess or tic-tac-toe or different arrow keys or buttons in a video game, etc. It represents the different options the agent has and can play at a given point in time. <em>A</em> represents the set of all unique actions available to the agent.</li>
</ul>
<p>It is worth mentioning that both <em>S</em> and <em>A</em> can be finite or infinite sets. Both can also be continuous spaces or discrete spaces, depending on the variables in the state space or the nature of actions. For example, if we have a variable taking real numbered values in the state space, the state space is automatically continuous (and infinite). If our actions are in the form of degree of turning a steering wheel, once again, continuous and infinite action space. A <em>finite MDP</em> has both finite <em>S</em> and finite <em>A</em>.</p>
<p>Before we move on to the other symbols, lets get some things clear. Here is another more detailed MDP for your reference.</p>



  











<figure>


  <a data-fancybox="" href="/img/post2_mdp.png" data-caption="Another MDP. Source: Google Images">
<img src="/img/post2_mdp.png" alt="" ></a>


  
  
  <figcaption>
    Another MDP. Source: Google Images
  </figcaption>


</figure>

<p>Some of the states of the MDP are designated as start states or initial states and end states or terminal states. An <em>episode</em> in RL is a sequence of state-action pairs that take the agent from a start state to a terminal state. So the agent starts from one of the intial states, plays an action, goes to the next state and so on until it hits a terminal state and the episode ends. Assume for now that there is such an end i.e. every episode does end after some unknown, finite time. This is a property of <em>finite horizon MDPs</em> which is what we will stick to in this course.</p>
<p>Now lets take a look at this MDP in the diagram above. Assume <em>S</em><!-- raw HTML omitted -->0<!-- raw HTML omitted --> is your initial state. Notice that <em>a</em><!-- raw HTML omitted -->0<!-- raw HTML omitted --> from <em>S</em><!-- raw HTML omitted -->0<!-- raw HTML omitted --> has two arrows, one going into <em>S</em><!-- raw HTML omitted -->0<!-- raw HTML omitted --> again and another going into <em>S</em><!-- raw HTML omitted -->2<!-- raw HTML omitted -->. The numbers on the arrows indicate 0.5 and 0.5 respectively. This means that if an agent plays the action <em>a</em><!-- raw HTML omitted -->0<!-- raw HTML omitted --> from <em>S</em><!-- raw HTML omitted -->0<!-- raw HTML omitted -->, it has a 0.5 probability that it ends up back in <em>S</em><!-- raw HTML omitted -->0<!-- raw HTML omitted --> and a 0.5 probability that it ends up in <em>S</em><!-- raw HTML omitted -->2<!-- raw HTML omitted -->. And similarly we have arrows going all over the diagram. Also notice the wiggly arrows &ndash; they&rsquo;re rewards.</p>
<p>Notationally, we index the sequence of state-action pairs in an episode with a time variable <em>t</em> so we say an agent plays action <em>a</em><!-- raw HTML omitted -->t<!-- raw HTML omitted --> from state <em>s</em><!-- raw HTML omitted -->t<!-- raw HTML omitted --> and gets reward <em>r</em><!-- raw HTML omitted -->t+1<!-- raw HTML omitted --> for doing so (the reward can be 0). Here, <em>t</em> starts from 0 and we represent the terminal time-step as <em>T</em>.</p>
<p>Another crucial thing to note is that actions in an MDP have to be instantaneous in nature. That means you take action <em>a</em><!-- raw HTML omitted -->t<!-- raw HTML omitted --> from state <em>s</em><!-- raw HTML omitted -->t<!-- raw HTML omitted --> and end up in state <em>s</em><!-- raw HTML omitted -->t+1<!-- raw HTML omitted --> immediately. There are several actions in real-world problems that may not be instantaneous but we&rsquo;ll deal with them later. For now, assume your actions are instantaneous.</p>
<ul>
<li><em>P</em> : Now P is the probability function defined as <em>P</em>(s<!-- raw HTML omitted -->'<!-- raw HTML omitted -->| s, a) which is read as the probability of moving to &ldquo;state s'&rdquo; from &ldquo;state s&rdquo; if the agent plays &ldquo;action a&rdquo;. So for example, <em>P</em>(<em>S</em><!-- raw HTML omitted -->2<!-- raw HTML omitted -->|<em>S</em><!-- raw HTML omitted -->0<!-- raw HTML omitted -->,<em>a</em><!-- raw HTML omitted -->0<!-- raw HTML omitted -->) = 0.5. This is also called the <em>transition function</em>.</li>
<li><em>R</em> : This is the <em>reward function</em> or <em>reinforcement function</em> and is defined as <em>R</em>(s<!-- raw HTML omitted -->'<!-- raw HTML omitted -->| s, a) which is the reward the agent gets for moving to &ldquo;state s'&rdquo; from &ldquo;state s&rdquo; if the agent plays &ldquo;action a&rdquo;. So for example, <em>R</em>(<em>S</em><!-- raw HTML omitted -->0<!-- raw HTML omitted -->|<em>S</em><!-- raw HTML omitted -->1<!-- raw HTML omitted -->,<em>a</em><!-- raw HTML omitted -->0<!-- raw HTML omitted -->) = +5. However, the answer may not be as simple as a single number and the reward could be sampled from a probability distribution i.e. <em>R</em>(s<!-- raw HTML omitted -->'<!-- raw HTML omitted -->| s, a) is a probability distribution that we will try to estimate by interaction with the environment. There are some other forms of reward functions like <em>R</em>(s,a) or even just <em>R</em>(s) as well. Note that rewards are always scalars because they are easier to optimize. They are also usually finite.</li>
<li>$\gamma$ : We spoke about the concept of delayed rewards earlier in the post and we wanted a way to accound for delayed effects of actions. This is where $\gamma$ helps. We define the <em>returns</em> of an action from a given state as the sum of the <em>discounted rewards</em> we receive from that state for playing that action. If we started from the state <em>s</em><!-- raw HTML omitted -->0<!-- raw HTML omitted -->, the returns would be defined as <em>r</em><!-- raw HTML omitted -->1<!-- raw HTML omitted --> + $\gamma$ <em>r</em><!-- raw HTML omitted -->2<!-- raw HTML omitted --> + $\gamma$<!-- raw HTML omitted -->2<!-- raw HTML omitted --> <em>r</em><!-- raw HTML omitted -->3<!-- raw HTML omitted --> + &hellip; $\gamma$ <!-- raw HTML omitted --><em>T-1</em><!-- raw HTML omitted --> <em>r</em><!-- raw HTML omitted --><em>T</em><!-- raw HTML omitted -->. We use the word &ldquo;discounted&rdquo; because $\gamma$ is usually a number between 0 and 1 and with the increasing powers, we give more weight to the immediate rewards than the delayed rewards. Hence, $\gamma$ is also called the discounting factor. The symbol we use for returns from timestep <em>t</em> is usually <em>G</em><!-- raw HTML omitted --><em>t</em><!-- raw HTML omitted --> although some people like using <em>R</em> as well (<em>r</em> for reward and <em>R</em> for returns). We will stick to the former notation. Now going back to the queen sacrifice example, if we were to consider the returns in our algorithm instead of just the immediate reward, we will be able to account for the delayed positive effect and not just the immediate negative effect.</li>
</ul>
<p>But with all of that information, we still haven&rsquo;t covered that blockquote at the start of this section. No, that wasn&rsquo;t a quote from Avengers: Endgame and it is the most important takeaway from this post. So what does it mean? Mathematically, it means this:</p>
<p>$$P(s_{t+1}|s_t,a) = P(s_{t+1}|s_t,s_{t-1},s_{t-2},s_{t-3},&hellip;,a)$$</p>
<p>But what does that mean intuitively? It means the probability of going to state <em>s</em><!-- raw HTML omitted -->t+1<!-- raw HTML omitted --> from state <em>s</em><!-- raw HTML omitted -->t<!-- raw HTML omitted --> under the action <em>a</em> is independent of how we got to state <em>s</em><!-- raw HTML omitted -->t<!-- raw HTML omitted --> in the first place. If you&rsquo;re thinking this isn&rsquo;t very realistic, you&rsquo;re right but this type of modeling works in most cases and is called <em>The Markov Property</em>. The future is independent of the past, given the present i.e. the action from the current state only depends on the current state and not how we got to this state in the first place.</p>
<p>We make another assumption of stationarity which means:</p>
<p>$$P(s_{t+1}=s',r_{t+1}=r|s_{t}=s,a_{t}=a) = P(s',r|s,a)$$</p>
<p>This means that the timestep variable doesn&rsquo;t matter &ndash; no matter at what point we are in the trajectory, given the state <em>s</em> and and action <em>a</em>, we always end up sampling the next state <em>s'</em> and the reward <em>r</em> from the same distribution. This is a property of <em>stationary</em> MDPs.</p>
<p>And with that, we&rsquo;ve covered MDPs and how to model RL problems. With this background, I also recommend reading <a href="https://skandavaidyanath.github.io/post/modeling-rl-problems/">this</a>. But with that definition, we still haven&rsquo;t accounted for the exploration-exploitation tradeoff. So in the <a href="https://skandavaidyanath.github.io/courses/rl-course/post3/">next post</a>, we&rsquo;ll introduce a few more symbols and definitions before we get cracking with our very first RL algorithm!</p>
<p>Once again, let me know if you have any feedback or suggestions.</p>
<h3 id="references">References</h3>
<ol>
<li><a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html">A (Long) Peek into Reinforcement Learning by Lilian Weng</a></li>
<li><a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">David Silver&rsquo;s RL Course at UCL</a></li>
<li>For MDP notation, <a href="https://ai.stanford.edu/~ang/papers/icml00-irl.pdf">Algorithms for Inverse Reinforcement Learning</a></li>
</ol>

          </div>

          



          
          <div class="article-widget">
            
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/courses/rl-course/post1/" rel="next">Introduction: Why RL?</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/courses/rl-course/post3/" rel="prev">Policies, Value Functions and the Bellman Equation</a>
  </div>
  
</div>

          </div>
          
        </div>

        <div class="body-footer">
          <p>Last updated on May 5, 2019</p>

          
<p class="edit-page">
  <a href="https://github.com/gcushen/hugo-academic/edit/master/content/courses/rl-course/post2.md">
    <i class="fas fa-pen pr-2"></i>Edit this page
  </a>
</p>



          

        </div>

      </article>

      <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
  </p>
</footer>


    </main>
  </div>
</div>


      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.1/anchor.min.js" integrity="sha256-pB/deHc9CGfFpJRjC43imB29Rse8tak+5eXqntO94ck=" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.c5b98a8914247ae95edfbe976665d60f.js"></script>

    






  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
